{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "mmm_overview_header"
      },
      "source": [
        "# Advanced Marketing Mix Modeling (MMM) Pipeline\n",
        "## Global B2B Multi-Regional, Multi-Product Implementation\n",
        "\n",
        "### What This Notebook Does\n",
        "\n",
        "**Marketing Mix Modeling (MMM)** answers: *\"For every dollar we spend on LinkedIn vs. Google vs. Display, how much revenue do we get back?\"*\n",
        "\n",
        "Unlike digital attribution (last-click), MMM uses **statistical regression** to estimate causal effects of media spend on revenue, accounting for:\n",
        "- **Time delays** (spend today → revenue in 6-9 months for B2B)\n",
        "- **Diminishing returns** (doubling spend doesn't double results)\n",
        "- **External factors** (seasonality, economic conditions)\n",
        "\n",
        "### Core Techniques (cells 4-7)\n",
        "\n",
        "| Technique | What It Does | Why We Need It |\n",
        "|-----------|--------------|----------------|\n",
        "| **Geometric Adstock** | Spreads each week's spend across future weeks with exponential decay | LinkedIn ads today still influence buyers 4 weeks from now |\n",
        "| **Hill Saturation** | S-curve transformation that flattens at high spend | The 10th impression to the same person has ~0 value |\n",
        "| **Nevergrad Optimization** | Evolutionary algorithm to find best decay/saturation params | Can't grid-search 60 parameters; no gradients available |\n",
        "| **Ridge Regression** | L2-penalized linear model with positive constraints | Prevents overfitting; ensures \"more spend ≠ less revenue\" |\n",
        "\n",
        "### Validation & Uncertainty (cells 6, 8-9)\n",
        "\n",
        "| Feature | Purpose |\n",
        "|---------|---------|\n",
        "| **Time-Series CV** | Tests if model can predict FUTURE quarters (not just fit history) |\n",
        "| **Bootstrap CI** | Shows \"LinkedIn ROI is 3.2x [2.8-3.6]\" not just \"3.2x\" |\n",
        "| **MAPE Metric** | \"We're typically 12% off\" is more intuitive than R² |\n",
        "\n",
        "### Output (cells 12-13)\n",
        "\n",
        "Results are saved to `ATOMIC.MMM_MODEL_RESULT` with ROI, confidence intervals, marginal ROI, and recommended spend per Channel × Region × Product.\n",
        "\n",
        "---\n",
        "**Data Sources**\n",
        "- **Input**: `DIMENSIONAL.V_MMM_INPUT_WEEKLY` (weekly spend + revenue + controls)\n",
        "- **Output**: `ATOMIC.MMM_MODEL_RESULT` (joins to dimension tables via FK)\n",
        "\n",
        "**Infrastructure**: Snowflake Notebooks or SPCS | Python 3.11 | ~5-10 min runtime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "install_packages"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 0: Install Required Packages\n",
        "# =============================================================================\n",
        "# Note: nevergrad and snowflake-ml-python are not pre-installed in Snowflake notebooks\n",
        "# This cell requires EXTERNAL_ACCESS_INTEGRATIONS to be configured\n",
        "# Using os.system() for compatibility with headless SPCS execution\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "packages = [\"nevergrad\", \"snowflake-ml-python\"]\n",
        "for pkg in packages:\n",
        "    print(f\"Installing {pkg}...\")\n",
        "    result = os.system(f\"{sys.executable} -m pip install {pkg} -q\")\n",
        "    print(f\"  {'✓ Done' if result == 0 else '⚠ Failed'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "imports_and_config"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 1: Imports and Configuration\n",
        "# =============================================================================\n",
        "# \n",
        "# KEY LIBRARIES:\n",
        "# - nevergrad: Meta's derivative-free optimization library. We use it because\n",
        "#   MMM hyperparameters (adstock decay, saturation) don't have clean gradients.\n",
        "#   TwoPointsDE is an evolutionary algorithm that works well for ~10-100 params.\n",
        "#\n",
        "# - Ridge regression: Linear model with L2 penalty. We use Ridge (not OLS) because:\n",
        "#   (1) Marketing data is often collinear (channels spike together in Q4)\n",
        "#   (2) L2 shrinks coefficients toward zero, preventing wild estimates\n",
        "#   (3) We can't use Lasso (L1) because it zeros out channels entirely\n",
        "#\n",
        "# - scipy.optimize: For budget reallocation with business constraints (±30% limits)\n",
        "#\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# LIBRARY CONTEXT FOR MAINTAINERS\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "#\n",
        "# pandas/numpy: Standard data manipulation. No special versions required.\n",
        "#\n",
        "# sklearn.linear_model.Ridge:\n",
        "#   - Implements: β = (X'X + λI)⁻¹X'y (closed-form, fast)\n",
        "#   - alpha parameter = λ in the penalty term\n",
        "#   - We use default alpha=1.0 which is mild regularization\n",
        "#\n",
        "# sklearn.preprocessing.StandardScaler:\n",
        "#   - Transforms X to zero mean, unit variance: X_scaled = (X - μ) / σ\n",
        "#   - Critical for Ridge: penalty treats all features equally, so they must\n",
        "#     be on the same scale. Without scaling, a $1M spend column would have\n",
        "#     tiny coefficients vs. a [0,1] saturation column with huge coefficients.\n",
        "#\n",
        "# nevergrad:\n",
        "#   - Meta's library for \"black-box\" optimization (no gradients needed)\n",
        "#   - TwoPointsDE: Differential Evolution variant, population-based\n",
        "#   - \"budget\" = number of function evaluations (not $$ budget!)\n",
        "#   - See: https://facebookresearch.github.io/nevergrad/\n",
        "#\n",
        "# scipy.optimize.minimize:\n",
        "#   - General-purpose minimizer with many algorithms\n",
        "#   - method='SLSQP': Sequential Least Squares Programming\n",
        "#     Handles equality + inequality constraints efficiently\n",
        "#   - We use it for budget allocation where SLSQP's constraint handling shines\n",
        "#\n",
        "# ALTERNATIVES CONSIDERED (not used):\n",
        "# - PyMC/Stan: Bayesian MMM (e.g., Robyn). More principled uncertainty but\n",
        "#   10x slower and requires MCMC tuning expertise.\n",
        "# - LightweightMMM (Google): Similar approach but JAX-based. We chose sklearn\n",
        "#   for broader compatibility with Snowflake's Python environment.\n",
        "# - CausalImpact: Good for single interventions, not continuous spend allocation.\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass, field\n",
        "import json\n",
        "\n",
        "# Snowflake\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.snowpark import functions as F\n",
        "\n",
        "# ML/Stats\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "import nevergrad as ng\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)  # Reproducibility for bootstrap sampling\n",
        "\n",
        "# Configuration\n",
        "@dataclass\n",
        "class MMMConfig:\n",
        "    \"\"\"\n",
        "    Configuration for MMM model training.\n",
        "    \n",
        "    GRANULARITY CHOICES:\n",
        "    - geo_level: Controls geographic aggregation. Options:\n",
        "        - \"GLOBAL\": Aggregate all regions (best for sparse data, ~10 channel groups)\n",
        "        - \"SUPER_REGION\": 3-4 regions per channel (needs 50+ weeks per combo)\n",
        "        - \"REGION\" or \"COUNTRY\": More granular (needs very rich data)\n",
        "      Rule of thumb: need ~50+ weeks of non-zero REVENUE per Channel×Geo combo.\n",
        "    \n",
        "    - product_level: SEGMENT (4 groups) vs CATEGORY (23 groups). More granular = \n",
        "      more actionable but requires more data. Start with SEGMENT, drill down if R² holds.\n",
        "    \n",
        "    HYPERPARAMETER SEARCH:\n",
        "    - nevergrad_budget: 500 iterations is a good balance. Robyn uses 2000+ but we're\n",
        "      optimizing fewer params (no decomposition). Increase if CV MAPE is unstable.\n",
        "    \n",
        "    VALIDATION:\n",
        "    - cv_train_weeks=52: Full year captures seasonality (Q1 budget flush, Q4 holidays)\n",
        "    - cv_test_weeks=13: Quarter-out holdout mimics real forecasting use case\n",
        "    \n",
        "    DATA SPARSITY NOTE:\n",
        "    If CV MAPE > 50%, the data is likely too sparse for the chosen granularity.\n",
        "    Switch geo_level to \"GLOBAL\" to aggregate across regions and improve model stability.\n",
        "    \"\"\"\n",
        "    # Data sources\n",
        "    input_view: str = \"DIMENSIONAL.V_MMM_INPUT_WEEKLY\"\n",
        "    output_table: str = \"ATOMIC.MMM_MODEL_RESULT\"\n",
        "    \n",
        "    # Model granularity - use GLOBAL for channel-only modeling (most robust)\n",
        "    # Use SUPER_REGION only if you have 50+ weeks with revenue per channel-region\n",
        "    geo_level: str = \"GLOBAL\"         # GLOBAL (recommended), SUPER_REGION, REGION, or COUNTRY\n",
        "    product_level: str = \"SEGMENT\"    # SEGMENT, DIVISION, or CATEGORY\n",
        "    \n",
        "    # Hyperparameter optimization\n",
        "    nevergrad_budget: int = 500       # Evolutionary algorithm iterations\n",
        "    # ridge_alpha: float = 10.0       # DEMO: Lower regularization → wilder ROI estimates (try this to show overfitting)\n",
        "    ridge_alpha: float = 50.0         # L2 penalty strength (stronger regularization → more conservative, realistic ROI)\n",
        "    \n",
        "    # Time-series cross-validation (rolling window, never peek at future)\n",
        "    cv_train_weeks: int = 52          # 1 year training window\n",
        "    cv_test_weeks: int = 13           # 1 quarter holdout (13 weeks)\n",
        "    cv_step_weeks: int = 13           # Roll forward 1 quarter between folds\n",
        "    \n",
        "    # Bootstrap for uncertainty quantification\n",
        "    n_bootstrap: int = 100            # Resample iterations (100 is standard)\n",
        "    confidence_level: float = 0.90    # 90% CI = 5th to 95th percentile\n",
        "    \n",
        "    # Budget optimizer constraints\n",
        "    budget_change_limit: float = 0.30 # ±30% per channel (realistic for CMO approval)\n",
        "    \n",
        "    # Model versioning (for tracking in MMM_MODEL_RESULT table and Model Registry)\n",
        "    # Uses semantic prefix + timestamp for auto-increment: v3_1_YYYYMMDD_HHMMSS\n",
        "    _version_prefix: str = \"v3_1\"\n",
        "    model_version: str = field(default_factory=lambda: f\"v3_1_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
        "\n",
        "config = MMMConfig()\n",
        "print(f\"MMM Configuration initialized: {config.model_version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "load_data_from_snowflake"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 2: Connect to Snowflake and Load Data\n",
        "# =============================================================================\n",
        "\n",
        "session = get_active_session()\n",
        "print(f\"Connected to Snowflake: {session.get_current_database()}.{session.get_current_schema()}\")\n",
        "\n",
        "# Load weekly aggregated data from dimensional view\n",
        "df_raw = session.table(config.input_view).to_pandas()\n",
        "\n",
        "# Standardize column names to uppercase\n",
        "df_raw.columns = df_raw.columns.str.upper()\n",
        "\n",
        "# Map view column names to expected model column names\n",
        "# The view uses _NAME/_CODE suffixes, but the model expects simple names\n",
        "column_mapping = {\n",
        "    'SUPER_REGION_NAME': 'SUPER_REGION',\n",
        "    'REGION_NAME': 'REGION',\n",
        "    'COUNTRY_NAME': 'COUNTRY',\n",
        "    'SEGMENT_NAME': 'SEGMENT',\n",
        "    'DIVISION_NAME': 'DIVISION',\n",
        "    'CATEGORY_NAME': 'CATEGORY',\n",
        "    'CHANNEL_CODE': 'CHANNEL',\n",
        "    'AVG_PMI': 'PMI_INDEX',\n",
        "    'AVG_COMPETITOR_SOV': 'COMPETITOR_SOV',\n",
        "    'AVG_INDUSTRY_GROWTH': 'INDUSTRY_GROWTH'\n",
        "}\n",
        "df_raw = df_raw.rename(columns=column_mapping)\n",
        "\n",
        "print(f\"\\nLoaded {len(df_raw):,} rows from {config.input_view}\")\n",
        "print(f\"Date range: {df_raw['WEEK_START'].min()} to {df_raw['WEEK_START'].max()}\")\n",
        "print(f\"\\nColumns: {df_raw.columns.tolist()}\")\n",
        "\n",
        "# Check dimension coverage\n",
        "print(f\"\\nDimension coverage:\")\n",
        "print(f\"  SUPER_REGION: {df_raw['SUPER_REGION'].dropna().unique().tolist()}\")\n",
        "print(f\"  CHANNEL: {df_raw['CHANNEL'].dropna().unique().tolist()}\")\n",
        "segment_vals = df_raw['SEGMENT'].dropna().unique().tolist() if 'SEGMENT' in df_raw.columns and df_raw['SEGMENT'].notna().any() else []\n",
        "print(f\"  SEGMENT: {segment_vals if segment_vals else 'All NULL - will use ALL'}\")\n",
        "\n",
        "# Preview data\n",
        "df_raw.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "prepare_mmm_data_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 3: Data Preparation and Feature Engineering\n",
        "# =============================================================================\n",
        "#\n",
        "# WHY THESE FEATURES MATTER:\n",
        "#\n",
        "# 1. COMPOSITE KEYS (Channel_Region_Product):\n",
        "#    We model each combination separately because \"LinkedIn in EMEA for Safety\"\n",
        "#    behaves differently than \"LinkedIn in APAC for Healthcare\". This is the core\n",
        "#    value prop: granular ROI, not just \"LinkedIn overall\".\n",
        "#\n",
        "# 2. FOURIER TERMS FOR SEASONALITY:\n",
        "#    Instead of 52 dummy variables (one per week), we use sin/cos waves.\n",
        "#    - SIN_1/COS_1: Annual cycle (captures \"Q4 always high\")\n",
        "#    - SIN_2/COS_2: Semi-annual (captures \"Q2 and Q4 different from Q1/Q3\")\n",
        "#    - SIN_3/COS_3: Quarterly patterns\n",
        "#    Benefit: 6 features instead of 52, prevents overfitting, captures smooth patterns.\n",
        "#\n",
        "# 3. TREND COMPONENT:\n",
        "#    Linear time trend captures \"revenue grows 5% per year regardless of marketing\".\n",
        "#    Without this, model would attribute organic growth to whichever channel scaled up.\n",
        "#\n",
        "# 4. Q1/Q3 FLAGS:\n",
        "#    B2B-specific: Q1 = budget flush, Q3 = pre-year-end push. Binary flags let the\n",
        "#    model learn \"Q1 has 20% higher baseline\" without needing exact week-of-year.\n",
        "#\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# DEEPER DIVE: FOURIER SEASONALITY (From Signal Processing)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "#\n",
        "# Any periodic signal can be decomposed into a sum of sinusoids (Fourier theorem).\n",
        "# For a signal with period T (52 weeks for annual seasonality):\n",
        "#\n",
        "#   f(t) = a₀ + Σₖ [aₖ·cos(2πkt/T) + bₖ·sin(2πkt/T)]\n",
        "#\n",
        "# Each k represents a \"harmonic\" of the fundamental frequency:\n",
        "#   k=1: Fundamental (one complete cycle per year)\n",
        "#   k=2: First harmonic (two cycles per year, i.e., semi-annual)\n",
        "#   k=3: Second harmonic (four cycles, i.e., quarterly)\n",
        "#\n",
        "# WHY SIN AND COS PAIRS:\n",
        "# A single sinusoid aₖ·cos(2πkt/T + φ) has both amplitude aₖ and phase φ.\n",
        "# Using both sin AND cos with separate coefficients, the regression can learn\n",
        "# any amplitude and phase:\n",
        "#   aₖ·cos(2πkt/T + φ) = (aₖ·cos(φ))·cos(2πkt/T) + (aₖ·sin(φ))·sin(2πkt/T)\n",
        "#                       = β_cos·cos(2πkt/T) + β_sin·sin(2πkt/T)\n",
        "#\n",
        "# So the model learns β_cos and β_sin, and we get:\n",
        "#   Amplitude = √(β_cos² + β_sin²)\n",
        "#   Phase = arctan(β_sin / β_cos)\n",
        "#\n",
        "# WHY 3 HARMONICS:\n",
        "# - k=1,2,3 capture annual, semi-annual, and quarterly patterns\n",
        "# - Higher harmonics (k=4,5,...) would capture weekly fluctuations\n",
        "# - More harmonics = more flexible but higher overfitting risk\n",
        "# - Rule of thumb: use k = 1 to (n/2-1) where n = periods per year ÷ 10\n",
        "#   For weekly data with 52 periods/year: up to k ≈ 3 is reasonable\n",
        "#\n",
        "# ALTERNATIVE: SEASONAL DUMMIES\n",
        "# 52 binary indicators (one per week) is equivalent to unlimited harmonics.\n",
        "# Downsides: 52 extra parameters, can't extrapolate, captures noise not signal.\n",
        "# =============================================================================\n",
        "\n",
        "def prepare_mmm_data(df: pd.DataFrame, config: MMMConfig) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepare data for MMM modeling with proper granularity and features.\n",
        "    \n",
        "    Features added:\n",
        "    - Composite keys for channel × region × product (granular attribution)\n",
        "    - Fourier seasonality (smooth annual/semi-annual patterns, 6 features vs 52 dummies)\n",
        "    - Linear trend (isolate organic growth from marketing impact)\n",
        "    - B2B fiscal flags (Q1 budget flush, Q3 push)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Ensure datetime\n",
        "    df['WEEK_START'] = pd.to_datetime(df['WEEK_START'])\n",
        "    \n",
        "    # Fill missing values\n",
        "    numeric_cols = ['SPEND', 'IMPRESSIONS', 'CLICKS', 'REVENUE', 'PMI_INDEX', 'COMPETITOR_SOV']\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(0)\n",
        "    \n",
        "    # Create composite dimension keys based on config granularity\n",
        "    geo_col = config.geo_level\n",
        "    prod_col = config.product_level\n",
        "    \n",
        "    # Handle GLOBAL geo_level - aggregate all regions into single \"GLOBAL\" value\n",
        "    # This is recommended for sparse data to ensure sufficient sample size per channel\n",
        "    if geo_col == \"GLOBAL\":\n",
        "        print(f\"  Using GLOBAL geo aggregation (channel-only modeling)\")\n",
        "        df['GEO_KEY'] = 'GLOBAL'\n",
        "        geo_col = 'GEO_KEY'\n",
        "    else:\n",
        "        print(f\"  Looking for geo_col='{geo_col}' in columns: {'YES' if geo_col in df.columns else 'NO'}\")\n",
        "        if geo_col in df.columns:\n",
        "            df[geo_col] = df[geo_col].fillna('UNKNOWN')\n",
        "        else:\n",
        "            df[geo_col] = 'ALL'\n",
        "            print(f\"  WARNING: {geo_col} column not found, using 'ALL'\")\n",
        "    \n",
        "    # Debug: print what columns we have\n",
        "    print(f\"  Looking for prod_col='{prod_col}' in columns: {'YES' if prod_col in df.columns else 'NO'}\")\n",
        "    print(f\"  Looking for 'CHANNEL' in columns: {'YES' if 'CHANNEL' in df.columns else 'NO'}\")\n",
        "        \n",
        "    if prod_col in df.columns:\n",
        "        df[prod_col] = df[prod_col].fillna('ALL')  # Use 'ALL' for null product since we don't have segment data\n",
        "    else:\n",
        "        df[prod_col] = 'ALL'\n",
        "        print(f\"  WARNING: {prod_col} column not found, using 'ALL'\")\n",
        "        \n",
        "    if 'CHANNEL' in df.columns:\n",
        "        df['CHANNEL'] = df['CHANNEL'].fillna('UNKNOWN')\n",
        "    else:\n",
        "        df['CHANNEL'] = 'UNKNOWN'\n",
        "        print(\"  WARNING: CHANNEL column not found!\")\n",
        "    \n",
        "    # Composite key: Channel_Region_Product\n",
        "    # With GLOBAL geo_level, this becomes Channel_GLOBAL_ALL (effectively channel-only)\n",
        "    df['CHANNEL_KEY'] = (\n",
        "        df['CHANNEL'].astype(str) + '_' + \n",
        "        df[geo_col].astype(str) + '_' + \n",
        "        df[prod_col].astype(str)\n",
        "    )\n",
        "    \n",
        "    # Add time features for seasonality\n",
        "    df['WEEK_OF_YEAR'] = df['WEEK_START'].dt.isocalendar().week.astype(int)\n",
        "    df['YEAR'] = df['WEEK_START'].dt.year\n",
        "    df['TREND'] = (df['WEEK_START'] - df['WEEK_START'].min()).dt.days / 365.25\n",
        "    \n",
        "    # Fourier terms for seasonality (annual cycle)\n",
        "    for k in [1, 2, 3]:\n",
        "        df[f'SIN_{k}'] = np.sin(2 * np.pi * k * df['WEEK_OF_YEAR'] / 52)\n",
        "        df[f'COS_{k}'] = np.cos(2 * np.pi * k * df['WEEK_OF_YEAR'] / 52)\n",
        "    \n",
        "    # Q1/Q3 seasonality flag (B2B budget cycles)\n",
        "    df['Q1_FLAG'] = ((df['WEEK_START'].dt.month >= 1) & (df['WEEK_START'].dt.month <= 3)).astype(int)\n",
        "    df['Q3_FLAG'] = ((df['WEEK_START'].dt.month >= 7) & (df['WEEK_START'].dt.month <= 9)).astype(int)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Prepare data\n",
        "df = prepare_mmm_data(df_raw, config)\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\nUnique channel keys: {df['CHANNEL_KEY'].nunique()}\")\n",
        "print(f\"Unique {config.geo_level}: {df[config.geo_level].nunique() if config.geo_level in df.columns else 'N/A'}\")\n",
        "print(f\"Unique {config.product_level}: {df[config.product_level].nunique() if config.product_level in df.columns else 'N/A'}\")\n",
        "print(f\"\\nSample channel keys: {df['CHANNEL_KEY'].unique()[:10]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "adstock_saturation_functions_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 4: Adstock and Saturation Transformation Functions\n",
        "# =============================================================================\n",
        "#\n",
        "# THESE ARE THE TWO CORE CONCEPTS IN MODERN MMM:\n",
        "#\n",
        "# ┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "# │ 1. ADSTOCK (Carryover Effect)                                               │\n",
        "# │                                                                             │\n",
        "# │ PROBLEM: A $100k LinkedIn campaign on Week 1 doesn't just affect Week 1.   │\n",
        "# │ In B2B, someone sees an ad, researches for 3 weeks, then converts.         │\n",
        "# │                                                                             │\n",
        "# │ SOLUTION: \"Spread\" the spend across future weeks with exponential decay:   │\n",
        "# │                                                                             │\n",
        "# │   Week 1: $100k    →  Effective: $100k                                     │\n",
        "# │   Week 2: $0       →  Effective: $70k  (70% of previous)                   │\n",
        "# │   Week 3: $0       →  Effective: $49k  (70% of $70k)                       │\n",
        "# │   Week 4: $0       →  Effective: $34k  ...and so on                        │\n",
        "# │                                                                             │\n",
        "# │ THETA PARAMETER:                                                            │\n",
        "# │   - theta = 0.0: No carryover (Search ads, immediate intent)               │\n",
        "# │   - theta = 0.5: Medium carryover (Facebook, consideration)                │\n",
        "# │   - theta = 0.8: Long carryover (LinkedIn B2B, 6-8 week cycles)            │\n",
        "# │   - theta = 0.95: Very long (TV brand campaigns, months of effect)         │\n",
        "# │                                                                             │\n",
        "# │ WHY GEOMETRIC: Simple (1 param), interpretable, matches empirical data.    │\n",
        "# │ Alternative: Weibull allows delayed peak (effect maxes at week 3).         │\n",
        "# └─────────────────────────────────────────────────────────────────────────────┘\n",
        "#\n",
        "# MATHEMATICAL INTUITION FOR ADSTOCK:\n",
        "#\n",
        "# The geometric adstock is a first-order autoregressive (AR(1)) filter:\n",
        "#   x_eff[t] = x[t] + θ·x_eff[t-1]\n",
        "#\n",
        "# Expanding recursively:\n",
        "#   x_eff[t] = x[t] + θ·x[t-1] + θ²·x[t-2] + θ³·x[t-3] + ...\n",
        "#\n",
        "# This is an infinite geometric series. For constant spend s, the steady-state is:\n",
        "#   x_eff_∞ = s·(1 + θ + θ² + ...) = s/(1-θ)\n",
        "#\n",
        "# Half-life interpretation: Effect decays to 50% after ln(0.5)/ln(θ) periods.\n",
        "#   θ=0.7 → half-life ≈ 1.9 weeks (effect halves in ~2 weeks)\n",
        "#   θ=0.9 → half-life ≈ 6.6 weeks (effect persists much longer)\n",
        "#\n",
        "# ┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "# │ 2. SATURATION / HILL FUNCTION (Diminishing Returns)                         │\n",
        "# │                                                                             │\n",
        "# │ PROBLEM: Doubling spend doesn't double revenue. At some point, you've      │\n",
        "# │ reached everyone interested. The 10th impression to the same person        │\n",
        "# │ has near-zero incremental value.                                           │\n",
        "# │                                                                             │\n",
        "# │ SOLUTION: S-curve transformation (Hill function from pharmacology):        │\n",
        "# │                                                                             │\n",
        "# │   Response │              ●●●●●●●●●●●●  ← Saturation (flat)               │\n",
        "# │      ▲     │         ●●●●●                                                 │\n",
        "# │      │     │      ●●●                    ← Efficient zone                  │\n",
        "# │      │     │    ●●                                                         │\n",
        "# │      │     │  ●●                                                           │\n",
        "# │      └─────┴──●───────────────────────► Spend                              │\n",
        "# │              gamma (half-saturation point)                                 │\n",
        "# │                                                                             │\n",
        "# │ PARAMETERS:                                                                 │\n",
        "# │   - alpha (shape): How steep the S-curve is. Higher = sharper transition.  │\n",
        "# │   - gamma (scale): Spend level where response = 50% of max.                │\n",
        "# │                    If gamma = $50k, then at $50k spend you're at 50%.      │\n",
        "# │                                                                             │\n",
        "# │ WHY HILL: Bounded [0,1], interpretable gamma, used by Robyn/LightweightMMM │\n",
        "# └─────────────────────────────────────────────────────────────────────────────┘\n",
        "#\n",
        "# MATHEMATICAL INTUITION FOR HILL SATURATION:\n",
        "#\n",
        "# The Hill function: f(x) = x^α / (x^α + γ^α)\n",
        "#\n",
        "# Key properties (useful for understanding marginal returns):\n",
        "#   - f(0) = 0, f(∞) → 1  (bounded response, asymptotes at max)\n",
        "#   - f(γ) = 0.5 exactly  (γ is the \"half-maximal effective dose\" or EC50)\n",
        "#   - Derivative: f'(x) = α·γ^α·x^(α-1) / (x^α + γ^α)²\n",
        "#     → Marginal response DECREASES as x increases (diminishing returns)\n",
        "#     → At x = γ, marginal response is α/(4γ)\n",
        "#\n",
        "# Alpha controls the \"steepness\" of the S-curve:\n",
        "#   - α < 1: Concave throughout (rapid early saturation, like commodity goods)\n",
        "#   - α = 1: Standard rectangular hyperbola f(x) = x/(x+γ) (Michaelis-Menten)\n",
        "#   - α > 1: Sigmoid with inflection point (threshold effect, then saturation)\n",
        "#            Inflection at x = γ·((α-1)/(α+1))^(1/α)\n",
        "#\n",
        "# The Hill function originates from enzyme kinetics (Michaelis-Menten) and\n",
        "# pharmacology (Hill coefficient for cooperative binding). In marketing, it\n",
        "# models audience saturation: eventually everyone who will respond, has.\n",
        "# =============================================================================\n",
        "\n",
        "def geometric_adstock(x: np.ndarray, theta: float) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Geometric Adstock Transformation (Carryover Effect).\n",
        "    \n",
        "    Models the \"memory\" of advertising: this week's effective spend includes\n",
        "    decayed contributions from all prior weeks. Equivalent to an infinite\n",
        "    geometric series: x_eff[t] = x[t] + θ*x[t-1] + θ²*x[t-2] + ...\n",
        "    \n",
        "    Formula: x_adstocked[t] = x[t] + theta * x_adstocked[t-1]\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    x : array - Raw spend values (weekly)\n",
        "    theta : float - Decay rate (0 to 1). The \"half-life\" is ln(0.5)/ln(θ) weeks.\n",
        "                   - LinkedIn B2B: 0.7-0.9 (long consideration cycle)\n",
        "                   - Paid Search: 0.1-0.3 (immediate intent, fast decay)\n",
        "                   - Display: 0.4-0.6 (awareness, medium decay)\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    x_adstocked : array - Transformed values reflecting cumulative exposure\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    x_adstocked = np.zeros_like(x)\n",
        "    \n",
        "    if len(x) == 0:\n",
        "        return x_adstocked\n",
        "    \n",
        "    # Recursive computation: each period inherits θ fraction of previous\n",
        "    x_adstocked[0] = x[0]\n",
        "    for t in range(1, len(x)):\n",
        "        x_adstocked[t] = x[t] + theta * x_adstocked[t-1]\n",
        "    \n",
        "    return x_adstocked\n",
        "\n",
        "\n",
        "def hill_saturation(x: np.ndarray, alpha: float, gamma: float) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Hill Function for Saturation (Diminishing Returns).\n",
        "    \n",
        "    Maps spend to a 0-1 scale representing \"response intensity\". At gamma spend,\n",
        "    response is exactly 0.5 (50% of maximum possible). This is the \"half-EC50\"\n",
        "    concept from pharmacology applied to marketing.\n",
        "    \n",
        "    Formula: x^α / (x^α + γ^α)\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    x : array - Adstocked spend values (apply adstock FIRST, then saturation)\n",
        "    alpha : float - Shape/slope parameter (typically 0.5 to 3.0)\n",
        "                   - alpha < 1: Concave from origin (quick saturation)\n",
        "                   - alpha = 1: Standard hyperbolic\n",
        "                   - alpha > 1: S-curve with inflection point (slow start, then steep)\n",
        "    gamma : float - Half-saturation point. Spend level where response = 50% of max.\n",
        "                   Typically set relative to observed spend range (e.g., median spend).\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    x_saturated : array - Values in [0, 1] representing response intensity\n",
        "    \n",
        "    Note: Final revenue contribution = coefficient × saturated_value\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    x = np.maximum(x, 0)  # No negative spend\n",
        "    gamma = max(gamma, 1e-10)  # Avoid division by zero\n",
        "    \n",
        "    # Hill function: asymptotes to 1 as x → ∞\n",
        "    x_saturated = (x ** alpha) / (x ** alpha + gamma ** alpha)\n",
        "    return x_saturated\n",
        "\n",
        "\n",
        "def apply_media_transformations(\n",
        "    X: pd.DataFrame, \n",
        "    params: Dict[str, Dict[str, float]], \n",
        "    channels: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Apply adstock and saturation transformations to all media channels.\"\"\"\n",
        "    X_transformed = X.copy()\n",
        "    \n",
        "    for ch in channels:\n",
        "        if ch not in X.columns or ch not in params:\n",
        "            continue\n",
        "            \n",
        "        p = params[ch]\n",
        "        x_raw = X[ch].values\n",
        "        \n",
        "        # Step 1: Adstock (carryover)\n",
        "        x_adstocked = geometric_adstock(x_raw, p['theta'])\n",
        "        \n",
        "        # Step 2: Saturation (diminishing returns)\n",
        "        x_saturated = hill_saturation(x_adstocked, p['alpha'], p['gamma'])\n",
        "        \n",
        "        X_transformed[ch] = x_saturated\n",
        "    \n",
        "    return X_transformed\n",
        "\n",
        "# Demonstrate transformations\n",
        "print(\"Transformation functions defined.\")\n",
        "print(\"\\nExample: Geometric adstock with theta=0.7\")\n",
        "sample_spend = np.array([100, 0, 0, 0, 0, 50, 0, 0])\n",
        "sample_adstock = geometric_adstock(sample_spend, theta=0.7)\n",
        "print(f\"Raw spend:     {sample_spend}\")\n",
        "print(f\"Adstocked:     {np.round(sample_adstock, 1)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "pivot_for_modeling_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 5: Pivot Data to Wide Format for Modeling\n",
        "# =============================================================================\n",
        "#\n",
        "# DATA SHAPE TRANSFORMATION:\n",
        "#\n",
        "# Input (long format):\n",
        "#   WEEK  |  CHANNEL_KEY           | SPEND  | REVENUE\n",
        "#   W1    |  LINKEDIN_EMEA_SI      | 50000  | 100000\n",
        "#   W1    |  GOOGLE_EMEA_SI        | 30000  | 100000\n",
        "#   W1    |  LINKEDIN_APAC_HC      | 20000  | 100000\n",
        "#   W2    |  LINKEDIN_EMEA_SI      | 45000  | 105000\n",
        "#   ...\n",
        "#\n",
        "# Output (wide format for regression):\n",
        "#   WEEK | LINKEDIN_EMEA_SI | GOOGLE_EMEA_SI | LINKEDIN_APAC_HC | ... | REVENUE\n",
        "#   W1   | 50000            | 30000          | 20000            | ... | 100000\n",
        "#   W2   | 45000            | 25000          | 22000            | ... | 105000\n",
        "#\n",
        "# WHY WIDE FORMAT:\n",
        "# Regression needs y ~ X1 + X2 + X3 + ...\n",
        "# Each column is a \"feature\" (channel×region×product combination)\n",
        "# Each row is an observation (week)\n",
        "#\n",
        "# MIN_SPEND_THRESHOLD:\n",
        "# Channels with < $1000 total spend are dropped because:\n",
        "#   - Not enough signal to estimate effect reliably\n",
        "#   - Adds noise and parameters without benefit\n",
        "#   - Can cause numerical instability in optimization\n",
        "# =============================================================================\n",
        "\n",
        "def pivot_for_modeling(\n",
        "    df: pd.DataFrame, \n",
        "    config: MMMConfig,\n",
        "    min_spend_threshold: float = 1000\n",
        ") -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, List[str]]:\n",
        "    \"\"\"\n",
        "    Pivot data to wide format for regression modeling.\n",
        "    \n",
        "    Transforms long-format data (one row per week×channel) to wide format\n",
        "    (one row per week, one column per channel). Filters out low-spend channels.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    X_media : DataFrame - Media spend variables (to be adstock/saturation transformed)\n",
        "    y : Series - Target variable (total revenue per week)\n",
        "    X_control : DataFrame - Control variables (seasonality, PMI, SOV)\n",
        "    channels : List - Channel keys with sufficient data for modeling\n",
        "    \"\"\"\n",
        "    # Aggregate by week and channel_key\n",
        "    df_agg = df.groupby(['WEEK_START', 'CHANNEL_KEY']).agg({\n",
        "        'SPEND': 'sum',\n",
        "        'IMPRESSIONS': 'sum',\n",
        "        'CLICKS': 'sum',\n",
        "        'REVENUE': 'sum'\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Pivot spend to wide format\n",
        "    X_media = df_agg.pivot_table(\n",
        "        index='WEEK_START',\n",
        "        columns='CHANNEL_KEY',\n",
        "        values='SPEND',\n",
        "        aggfunc='sum'\n",
        "    ).fillna(0).sort_index()\n",
        "    \n",
        "    # Filter channels with minimum spend\n",
        "    channel_totals = X_media.sum()\n",
        "    valid_channels = channel_totals[channel_totals >= min_spend_threshold].index.tolist()\n",
        "    X_media = X_media[valid_channels]\n",
        "    \n",
        "    # Target: Total revenue per week\n",
        "    y = df.groupby('WEEK_START')['REVENUE'].sum().sort_index()\n",
        "    \n",
        "    # Control variables\n",
        "    control_cols = ['TREND', 'SIN_1', 'COS_1', 'SIN_2', 'COS_2', 'Q1_FLAG', 'Q3_FLAG']\n",
        "    if 'PMI_INDEX' in df.columns:\n",
        "        control_cols.append('PMI_INDEX')\n",
        "    if 'COMPETITOR_SOV' in df.columns:\n",
        "        control_cols.append('COMPETITOR_SOV')\n",
        "    \n",
        "    X_control = df.groupby('WEEK_START')[control_cols].first().sort_index()\n",
        "    \n",
        "    # Align indices\n",
        "    common_idx = X_media.index.intersection(y.index).intersection(X_control.index)\n",
        "    X_media = X_media.loc[common_idx]\n",
        "    y = y.loc[common_idx]\n",
        "    X_control = X_control.loc[common_idx]\n",
        "    \n",
        "    channels = X_media.columns.tolist()\n",
        "    \n",
        "    return X_media, y, X_control, channels\n",
        "\n",
        "# Pivot data\n",
        "X_media, y, X_control, channels = pivot_for_modeling(df, config)\n",
        "\n",
        "print(f\"\\nModeling {len(channels)} channel-region-product combinations\")\n",
        "print(f\"Time periods: {len(y)} weeks\")\n",
        "print(f\"Total spend: ${X_media.sum().sum():,.0f}\")\n",
        "print(f\"Total revenue: ${y.sum():,.0f}\")\n",
        "print(f\"\\nControl variables: {X_control.columns.tolist()}\")\n",
        "print(f\"\\nTop 10 channels by spend:\")\n",
        "print(X_media.sum().sort_values(ascending=False).head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "cv_and_metrics_functions_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 6: Time-Series Cross-Validation\n",
        "# =============================================================================\n",
        "#\n",
        "# WHY TIME-SERIES CV INSTEAD OF K-FOLD:\n",
        "#\n",
        "# Standard k-fold CV randomly shuffles data, which would let us \"peek\" at future\n",
        "# weeks when predicting past weeks. This causes overly optimistic metrics because\n",
        "# the model learns patterns it wouldn't have access to in production.\n",
        "#\n",
        "# Time-series CV respects temporal order:\n",
        "#\n",
        "#   Fold 1: Train [Week 1-52]  → Test [Week 53-65]   (predict Q1 2024)\n",
        "#   Fold 2: Train [Week 14-65] → Test [Week 66-78]  (predict Q2 2024)\n",
        "#   Fold 3: Train [Week 27-78] → Test [Week 79-91]  (predict Q3 2024)\n",
        "#   ...\n",
        "#\n",
        "# This mimics real use: \"Given everything up to today, how well can we predict\n",
        "# next quarter?\" If CV MAPE is 12%, expect 12% error in actual forecasts.\n",
        "#\n",
        "# METRIC CHOICES:\n",
        "# - MAPE (Mean Absolute Percentage Error): \"On average, we're off by X%\"\n",
        "#   Industry standard for MMM. Target: <15% is good, <10% is excellent.\n",
        "# - NRMSE: RMSE normalized by mean. Comparable across different revenue scales.\n",
        "# - R²: Variance explained. >0.85 for in-sample, >0.70 for CV is solid.\n",
        "#\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# DEEPER DIVE: UNDERSTANDING EACH METRIC\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "#\n",
        "# R² (Coefficient of Determination):\n",
        "#   R² = 1 - SS_res / SS_tot = 1 - Σ(y - ŷ)² / Σ(y - ȳ)²\n",
        "#\n",
        "#   Interpretation: \"What fraction of the variance in y does our model explain?\"\n",
        "#   - R² = 1.0: Perfect predictions (ŷ = y for all points)\n",
        "#   - R² = 0.0: Model predicts the mean every time (useless)\n",
        "#   - R² < 0:   Model is WORSE than predicting the mean (possible in CV!)\n",
        "#\n",
        "#   Caution: R² can be artificially high if y has a strong trend. A model that\n",
        "#   just predicts \"revenue goes up 5% per year\" might get R² = 0.8 without\n",
        "#   capturing any marketing effects. That's why we include TREND as a control.\n",
        "#\n",
        "# RMSE (Root Mean Squared Error):\n",
        "#   RMSE = √[Σ(y - ŷ)² / n]\n",
        "#\n",
        "#   Same units as y (dollars). Penalizes large errors heavily due to squaring.\n",
        "#   Good for: \"On average, how many dollars off are we?\"\n",
        "#   Weakness: Not comparable across different revenue scales.\n",
        "#\n",
        "# MAE (Mean Absolute Error):\n",
        "#   MAE = Σ|y - ŷ| / n\n",
        "#\n",
        "#   More robust to outliers than RMSE (no squaring). Also in dollars.\n",
        "#   If MAE << RMSE, you have some big outliers (worth investigating).\n",
        "#\n",
        "# MAPE (Mean Absolute Percentage Error):\n",
        "#   MAPE = (1/n) · Σ|y - ŷ| / |y| × 100\n",
        "#\n",
        "#   The \"headline\" metric for business stakeholders.\n",
        "#   - Scale-free (%) so comparable across regions/products\n",
        "#   - Intuitive: \"We're typically 12% off\"\n",
        "#   - Weakness: Undefined when y = 0 (we mask those out)\n",
        "#   - Weakness: Asymmetric—50% under-prediction feels same as 100% over-prediction\n",
        "#\n",
        "# NRMSE (Normalized RMSE):\n",
        "#   NRMSE = RMSE / mean(y) × 100\n",
        "#\n",
        "#   Percentage scale like MAPE, but uses RMSE instead of MAE.\n",
        "#   Useful for comparing model quality across different datasets.\n",
        "#\n",
        "# WHY WE REPORT MULTIPLE METRICS:\n",
        "# No single metric tells the whole story. R² shows explanatory power, MAPE\n",
        "# shows practical accuracy, RMSE reveals if large errors exist. Together they\n",
        "# give a complete picture of model quality.\n",
        "# =============================================================================\n",
        "\n",
        "def time_series_cv_split(\n",
        "    n_samples: int,\n",
        "    train_size: int,\n",
        "    test_size: int,\n",
        "    step_size: int\n",
        ") -> List[Tuple[np.ndarray, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Generate time-series cross-validation splits (rolling window).\n",
        "    \n",
        "    Unlike k-fold, this NEVER lets the model see future data during training.\n",
        "    Each fold trains on [t, t+train_size) and tests on [t+train_size, t+train_size+test_size).\n",
        "    \n",
        "    With train=52, test=13, step=13:\n",
        "    - ~4 folds per 2 years of data\n",
        "    - Each fold predicts a full quarter ahead\n",
        "    - Realistic for \"next quarter budget planning\" use case\n",
        "    \"\"\"\n",
        "    splits = []\n",
        "    start = 0\n",
        "    while start + train_size + test_size <= n_samples:\n",
        "        train_idx = np.arange(start, start + train_size)\n",
        "        test_idx = np.arange(start + train_size, start + train_size + test_size)\n",
        "        splits.append((train_idx, test_idx))\n",
        "        start += step_size\n",
        "    return splits\n",
        "\n",
        "\n",
        "def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate regression metrics for model evaluation.\n",
        "    \n",
        "    Returns dict with:\n",
        "    - R²: Proportion of variance explained (1.0 = perfect, can be negative if worse than mean)\n",
        "    - RMSE: Root Mean Squared Error in dollars (same units as y)\n",
        "    - MAE: Mean Absolute Error in dollars (less sensitive to outliers than RMSE)\n",
        "    - MAPE: Mean Absolute Percentage Error (the \"headline\" metric for MMM)\n",
        "    - NRMSE: Normalized RMSE as % of mean (allows comparison across scales)\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    mask = y_true != 0  # Avoid division by zero in MAPE\n",
        "    \n",
        "    return {\n",
        "        'R2': r2_score(y_true, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "        'MAE': mean_absolute_error(y_true, y_pred),\n",
        "        'MAPE': np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if mask.sum() > 0 else np.nan,\n",
        "        'NRMSE': np.sqrt(mean_squared_error(y_true, y_pred)) / y_true.mean() * 100\n",
        "    }\n",
        "\n",
        "# Generate CV splits\n",
        "cv_splits = time_series_cv_split(\n",
        "    n_samples=len(y),\n",
        "    train_size=config.cv_train_weeks,\n",
        "    test_size=config.cv_test_weeks,\n",
        "    step_size=config.cv_step_weeks\n",
        ")\n",
        "\n",
        "print(f\"Time-Series Cross-Validation:\")\n",
        "print(f\"  Training window: {config.cv_train_weeks} weeks\")\n",
        "print(f\"  Test window: {config.cv_test_weeks} weeks\")\n",
        "print(f\"  Number of folds: {len(cv_splits)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "mmm_optimizer_class_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 7: Hyperparameter Optimization with Nevergrad\n",
        "# =============================================================================\n",
        "#\n",
        "# THE OPTIMIZATION PROBLEM:\n",
        "#\n",
        "# We need to find 3 hyperparameters PER CHANNEL:\n",
        "#   - theta (adstock decay): How quickly does ad effect fade?\n",
        "#   - alpha (saturation shape): How steep is the diminishing returns curve?\n",
        "#   - gamma (half-saturation): At what spend level do we hit 50% of max response?\n",
        "#\n",
        "# With 20 channels, that's 60 parameters. We can't grid search (60^10 = impossible).\n",
        "# We can't use gradient descent (the objective isn't smooth w.r.t. these params).\n",
        "#\n",
        "# SOLUTION: Evolutionary optimization (Nevergrad's TwoPointsDE)\n",
        "#\n",
        "# TwoPointsDE is a variant of Differential Evolution that:\n",
        "#   1. Starts with a population of random parameter guesses\n",
        "#   2. \"Breeds\" new guesses by combining good performers\n",
        "#   3. Keeps the best, discards the worst\n",
        "#   4. Repeats for `budget` iterations\n",
        "#\n",
        "# WHY 500 ITERATIONS: Empirically, loss stabilizes around 300-500 for this scale.\n",
        "# Robyn uses 2000+ but also optimizes decomposition. We're simpler.\n",
        "#\n",
        "# SIGMOID REPARAMETRIZATION:\n",
        "# Instead of letting optimizer search [0, 0.95] directly, we search [-5, 5] and\n",
        "# apply sigmoid to map to the valid range. This is a standard trick to:\n",
        "#   - Avoid boundary issues (optimizer doesn't get stuck at 0 or 0.95)\n",
        "#   - Make the search space more \"smooth\" for the evolutionary algorithm\n",
        "#\n",
        "# NEGATIVE COEFFICIENT PENALTY:\n",
        "# Economically, marketing should never HURT revenue. If the model wants to assign\n",
        "# a negative coefficient (e.g., \"more LinkedIn spend = less revenue\"), we penalize\n",
        "# this heavily. In practice, negative coefficients usually mean multicollinearity.\n",
        "#\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# DEEPER DIVE: WHY DERIVATIVE-FREE OPTIMIZATION?\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "#\n",
        "# The objective function L(θ, α, γ) = 1 - R²(model trained with those params)\n",
        "# involves FITTING A RIDGE REGRESSION INSIDE each evaluation. This creates\n",
        "# a \"nested\" optimization that makes gradients unavailable or meaningless:\n",
        "#\n",
        "#   ∂L/∂θ = ??? (how does R² change if we nudge decay by 0.01?)\n",
        "#\n",
        "# The relationship is:\n",
        "#   θ → adstock_transform(X) → Ridge.fit(X_transformed) → R²\n",
        "#\n",
        "# Each step involves discrete choices (which data points are \"influential\"),\n",
        "# matrix inversions, and nonlinear transforms. Autodiff doesn't help here.\n",
        "#\n",
        "# TwoPointsDE (Differential Evolution variant):\n",
        "# - Maintains a population of ~20-50 candidate solutions\n",
        "# - Creates new candidates via: x_new = x_a + F·(x_b - x_c) + noise\n",
        "#   where F is a mutation factor and a,b,c are random population members\n",
        "# - \"TwoPoints\" variant uses 2 random points for crossover, improving\n",
        "#   exploitation vs. exploration balance\n",
        "# - No gradients needed—just function evaluations and selection\n",
        "#\n",
        "# Alternatives considered:\n",
        "# - Bayesian Optimization: Better for <20 params, but O(n³) with iterations\n",
        "# - Random Search: Surprisingly effective, but needs 10x more iterations\n",
        "# - Hyperband: Good for early stopping, but our objective is fast to evaluate\n",
        "#\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# RIDGE REGRESSION: WHY L2 PENALTY?\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "#\n",
        "# Standard OLS minimizes: ||y - Xβ||²\n",
        "# Ridge adds:             ||y - Xβ||² + λ||β||²\n",
        "#\n",
        "# The L2 penalty (λ||β||²) does two things:\n",
        "# 1. REGULARIZATION: Shrinks coefficients toward zero, preventing overfitting\n",
        "#    when n (samples) is small relative to p (features). With 104 weeks and\n",
        "#    20+ channels, we're in moderate-dimensional territory.\n",
        "#\n",
        "# 2. MULTICOLLINEARITY FIX: When channels are correlated (LinkedIn and Display\n",
        "#    spike together in Q4), (X'X) is near-singular. Ridge adds λI to the diagonal:\n",
        "#    β_ridge = (X'X + λI)⁻¹X'y, which is always invertible.\n",
        "#\n",
        "# Why NOT Lasso (L1)? Lasso sets some coefficients exactly to zero, which is\n",
        "# great for feature selection but problematic here—we WANT every channel's\n",
        "# contribution estimated, even if small. Zeroing LinkedIn would lose insights.\n",
        "#\n",
        "# Why NOT ElasticNet? Adds complexity without clear benefit for our use case.\n",
        "# Ridge's closed-form solution is also computationally efficient.\n",
        "# =============================================================================\n",
        "\n",
        "class MMMOptimizer:\n",
        "    \"\"\"\n",
        "    Marketing Mix Model optimizer using Nevergrad evolutionary algorithm.\n",
        "    \n",
        "    Finds optimal (theta, alpha, gamma) for each channel by minimizing (1 - R²)\n",
        "    with a penalty for economically invalid negative coefficients.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, X_media, X_control, y, channels, config, observed_roas=None):\n",
        "        self.X_media = X_media\n",
        "        self.X_control = X_control\n",
        "        self.y = y\n",
        "        self.channels = channels\n",
        "        self.config = config\n",
        "        self.n_params = len(channels) * 3  # 3 params per channel\n",
        "        # Store max spend per channel for gamma scaling\n",
        "        self.channel_max = {ch: max(X_media[ch].max(), 1) for ch in channels}\n",
        "        # Store observed ROAS for each channel (used in ROI constraint)\n",
        "        self.observed_roas = observed_roas if observed_roas else {}\n",
        "        \n",
        "    def _decode_params(self, flat_params):\n",
        "        \"\"\"\n",
        "        Decode flat parameter array into structured dict.\n",
        "        \n",
        "        Uses sigmoid transform to map unbounded search space [-5, 5] to valid ranges:\n",
        "        - theta: [0, 0.95] (can't be 1.0 or adstock explodes)\n",
        "        - alpha: [0.5, 3.0] (reasonable S-curve shapes)\n",
        "        - gamma: [0, max_spend] (scaled to channel's observed range)\n",
        "        \"\"\"\n",
        "        params = {}\n",
        "        for i, ch in enumerate(self.channels):\n",
        "            base = i * 3\n",
        "            raw_theta, raw_alpha, raw_gamma = flat_params[base:base+3]\n",
        "            \n",
        "            # Sigmoid: 1/(1+e^-x) maps (-∞,∞) → (0,1), then scale to target range\n",
        "            theta = 1 / (1 + np.exp(-raw_theta)) * 0.95  # [0, 0.95]\n",
        "            alpha = 0.5 + 1 / (1 + np.exp(-raw_alpha)) * 2.5  # [0.5, 3.0]\n",
        "            gamma = 1 / (1 + np.exp(-raw_gamma)) * self.channel_max[ch]  # [0, max]\n",
        "            \n",
        "            params[ch] = {'theta': theta, 'alpha': alpha, 'gamma': max(gamma, 1e-6)}\n",
        "        return params\n",
        "    \n",
        "    def _objective(self, flat_params):\n",
        "        \"\"\"\n",
        "        Objective function: Minimize (1 - R²) + penalty for negative coefficients + ROI constraint.\n",
        "        \n",
        "        Why (1 - R²)? We want to MAXIMIZE R², but optimizers MINIMIZE.\n",
        "        So we minimize (1 - R²), which is 0 when R² = 1 (perfect fit).\n",
        "        \n",
        "        Why the penalties?\n",
        "        1. Negative coefficient penalty: Marketing spend should never decrease revenue.\n",
        "        2. ROI constraint penalty: Model ROI should be within reasonable range of observed ROAS.\n",
        "           This prevents the model from assigning unrealistic attribution (e.g., 48x ROI on TikTok).\n",
        "        \"\"\"\n",
        "        params = self._decode_params(flat_params)\n",
        "        X_media_trans = apply_media_transformations(self.X_media, params, self.channels)\n",
        "        X_full = pd.concat([X_media_trans, self.X_control], axis=1)\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_full)\n",
        "        \n",
        "        model = Ridge(alpha=self.config.ridge_alpha)\n",
        "        model.fit(X_scaled, self.y)\n",
        "        y_pred = model.predict(X_scaled)\n",
        "        \n",
        "        # Penalize negative media coefficients (economically invalid)\n",
        "        media_coefs = model.coef_[:len(self.channels)] / scaler.scale_[:len(self.channels)]\n",
        "        negative_penalty = np.sum(np.minimum(media_coefs, 0) ** 2) * 10\n",
        "        \n",
        "        # ROI constraint: penalize ROI estimates far from observed ROAS\n",
        "        # This keeps model attribution grounded in reality\n",
        "        # DEMO: Comment out this block (lines 161-174) to see unconstrained ROI estimates (e.g., 48x TikTok)\n",
        "        roi_penalty = 0.0\n",
        "        for i, ch in enumerate(self.channels):\n",
        "            coef = media_coefs[i]\n",
        "            contribution = X_media_trans[ch].sum() * coef\n",
        "            spend = self.X_media[ch].sum()\n",
        "            model_roi = contribution / spend if spend > 0 else 0\n",
        "            observed_roas = self.observed_roas.get(ch, 1.0)\n",
        "            \n",
        "            # Allow 3x deviation from observed ROAS before penalty kicks in\n",
        "            # DEMO: Change 3 to 10 for looser constraints, or 1.5 for tighter\n",
        "            max_allowed_roi = max(observed_roas * 3, 5.0)  # At least 5x to allow some flexibility\n",
        "            if model_roi > max_allowed_roi:\n",
        "                roi_penalty += ((model_roi - max_allowed_roi) / max_allowed_roi) ** 2\n",
        "        \n",
        "        roi_penalty *= 5  # Scale penalty weight (DEMO: Set to 0 to disable ROI constraint entirely)\n",
        "        \n",
        "        r2 = r2_score(self.y, y_pred)\n",
        "        return (1 - r2) + negative_penalty + roi_penalty\n",
        "    \n",
        "    def optimize(self, budget=500):\n",
        "        \"\"\"\n",
        "        Run Nevergrad optimization.\n",
        "        \n",
        "        TwoPointsDE (Two-Points Differential Evolution):\n",
        "        - Population-based evolutionary algorithm\n",
        "        - Creates new candidates by combining existing good solutions\n",
        "        - Robust to non-smooth, non-convex objective landscapes\n",
        "        - 500 iterations typically sufficient for 50-100 parameters\n",
        "        \"\"\"\n",
        "        print(f\"\\nOptimizing {self.n_params} parameters ({len(self.channels)} channels × 3 params)...\")\n",
        "        \n",
        "        # Search space: unbounded, will be mapped via sigmoid in _decode_params\n",
        "        parametrization = ng.p.Array(shape=(self.n_params,)).set_bounds(-5, 5)\n",
        "        optimizer = ng.optimizers.TwoPointsDE(parametrization=parametrization, budget=budget)\n",
        "        recommendation = optimizer.minimize(self._objective)\n",
        "        \n",
        "        best_params = self._decode_params(recommendation.value)\n",
        "        final_loss = self._objective(recommendation.value)\n",
        "        \n",
        "        print(f\"Optimization complete. Final loss: {final_loss:.4f} (R² ≈ {1 - final_loss:.4f})\")\n",
        "        return best_params, {'final_loss': final_loss}\n",
        "\n",
        "# Calculate observed ROAS for each channel (ground truth to constrain model)\n",
        "observed_roas = {}\n",
        "for ch in channels:\n",
        "    spend = X_media[ch].sum()\n",
        "    # Revenue is proportionally allocated by spend share\n",
        "    # Using y (total weekly revenue), we estimate channel contribution by spend proportion\n",
        "    revenue = df[df['CHANNEL_KEY'] == ch]['REVENUE'].sum() if 'REVENUE' in df.columns else 0\n",
        "    if spend > 0 and revenue > 0:\n",
        "        observed_roas[ch] = revenue / spend\n",
        "    else:\n",
        "        observed_roas[ch] = 1.0  # Default to breakeven if no data\n",
        "        \n",
        "print(f\"\\nObserved ROAS by channel (ground truth for ROI constraints):\")\n",
        "for ch, roas in sorted(observed_roas.items(), key=lambda x: -x[1])[:10]:\n",
        "    print(f\"  {ch}: {roas:.2f}x\")\n",
        "\n",
        "# Run optimization with ROI constraints\n",
        "optimizer = MMMOptimizer(X_media, X_control, y, channels, config, observed_roas=observed_roas)\n",
        "best_params, opt_metrics = optimizer.optimize(budget=config.nevergrad_budget)\n",
        "\n",
        "print(f\"\\nSample optimized parameters (first 5 channels):\")\n",
        "for ch in list(best_params.keys())[:5]:\n",
        "    p = best_params[ch]\n",
        "    print(f\"  {ch}: theta={p['theta']:.3f}, alpha={p['alpha']:.3f}, gamma={p['gamma']:.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "train_final_model_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 8: Final Model Training with Cross-Validation Metrics\n",
        "# =============================================================================\n",
        "#\n",
        "# TWO SEPARATE FITS:\n",
        "#\n",
        "# 1. IN-SAMPLE FIT (Full Data):\n",
        "#    - Train on ALL data, predict on ALL data\n",
        "#    - R² will be high (0.90+) because model has \"seen\" every data point\n",
        "#    - Use for: coefficient interpretation, response curves, budget optimization\n",
        "#\n",
        "# 2. CROSS-VALIDATION FIT (Rolling Window):\n",
        "#    - Train on past, predict on future, repeat\n",
        "#    - R² and MAPE will be WORSE than in-sample (this is expected!)\n",
        "#    - Use for: realistic accuracy estimate, \"will this work in production?\"\n",
        "#\n",
        "# WHY REPORT BOTH:\n",
        "#   - If in-sample R² = 0.95 but CV R² = 0.50, model is OVERFITTING\n",
        "#     (memorizing training data, not learning generalizable patterns)\n",
        "#   - Healthy gap: in-sample R² ≈ CV R² + 0.05-0.10\n",
        "#   - If gap is large: reduce model complexity (fewer channels, stronger ridge penalty)\n",
        "#\n",
        "# QUALITY THRESHOLDS (industry standard):\n",
        "#   CV MAPE < 10%: Excellent - model is highly predictive\n",
        "#   CV MAPE 10-20%: Good - suitable for budget optimization\n",
        "#   CV MAPE 20-30%: Acceptable - directional insights only\n",
        "#   CV MAPE > 30%: Poor - investigate data quality or model specification\n",
        "#\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# DEEPER DIVE: BIAS-VARIANCE TRADEOFF IN MMM\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "#\n",
        "# The gap between in-sample and CV performance illustrates the bias-variance\n",
        "# tradeoff central to all statistical learning:\n",
        "#\n",
        "#   Expected Error = Bias² + Variance + Irreducible Noise\n",
        "#\n",
        "# BIAS: Systematic error from model assumptions (e.g., assuming linearity when\n",
        "#       relationships are nonlinear). High bias = underfitting.\n",
        "#\n",
        "# VARIANCE: Sensitivity to the specific training data. A complex model might\n",
        "#           fit training data perfectly but fail on new data. High variance = overfitting.\n",
        "#\n",
        "# IN-SAMPLE ERROR captures (mostly) bias: if the model can't fit the training\n",
        "# data well, it has too little flexibility. Low in-sample error means the model\n",
        "# can represent the data's complexity.\n",
        "#\n",
        "# CV ERROR captures bias + variance: on unseen data, both systematic errors AND\n",
        "# overfitting manifest. The GAP between in-sample and CV is (roughly) variance.\n",
        "#\n",
        "# HOW RIDGE HELPS:\n",
        "# Ridge penalty λ||β||² adds bias (shrinks coefficients toward zero) but reduces\n",
        "# variance (prevents wild coefficient estimates from correlated features).\n",
        "#\n",
        "# If CV performance is poor despite good in-sample fit:\n",
        "#   - Increase λ (more regularization) to reduce variance\n",
        "#   - Reduce model complexity (fewer channels, simpler transforms)\n",
        "#   - Get more data (especially more time periods)\n",
        "#\n",
        "# If both in-sample AND CV are poor:\n",
        "#   - Model may be too simple (underfitting)\n",
        "#   - Check data quality (missing spend, misaligned time series)\n",
        "#   - Consider adding control variables (macro factors, seasonality)\n",
        "#\n",
        "# THE ±STD IN CV METRICS:\n",
        "# We report mean ± std across folds. High std indicates inconsistent performance\n",
        "# across time periods. This could mean:\n",
        "#   - Some quarters are inherently harder to predict (Q4 chaos)\n",
        "#   - Structural breaks (pandemic, new product launch)\n",
        "#   - Concept drift (marketing effectiveness changing over time)\n",
        "# =============================================================================\n",
        "\n",
        "def train_final_model(X_media, X_control, y, channels, params, cv_splits, config):\n",
        "    \"\"\"\n",
        "    Train final model and compute both in-sample and cross-validation metrics.\n",
        "    \n",
        "    In-sample metrics show model fit; CV metrics show predictive accuracy.\n",
        "    Large gap between them indicates overfitting.\n",
        "    \"\"\"\n",
        "    # Transform media with optimized hyperparameters\n",
        "    X_media_trans = apply_media_transformations(X_media, params, channels)\n",
        "    X_full = pd.concat([X_media_trans, X_control], axis=1)\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_full)\n",
        "    \n",
        "    # In-Sample Fit\n",
        "    model = Ridge(alpha=config.ridge_alpha)\n",
        "    model.fit(X_scaled, y)\n",
        "    y_pred_insample = model.predict(X_scaled)\n",
        "    insample_metrics = calculate_metrics(y.values, y_pred_insample)\n",
        "    \n",
        "    # Cross-Validation\n",
        "    cv_metrics_list = []\n",
        "    y_values = y.values\n",
        "    \n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(cv_splits):\n",
        "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
        "        y_train, y_test = y_values[train_idx], y_values[test_idx]\n",
        "        \n",
        "        cv_model = Ridge(alpha=config.ridge_alpha)\n",
        "        cv_model.fit(X_train, y_train)\n",
        "        y_pred = cv_model.predict(X_test)\n",
        "        \n",
        "        fold_metrics = calculate_metrics(y_test, y_pred)\n",
        "        fold_metrics['fold'] = fold_idx + 1\n",
        "        cv_metrics_list.append(fold_metrics)\n",
        "    \n",
        "    cv_metrics_df = pd.DataFrame(cv_metrics_list)\n",
        "    cv_metrics_avg = cv_metrics_df.drop('fold', axis=1).mean().to_dict()\n",
        "    cv_metrics_std = cv_metrics_df.drop('fold', axis=1).std().to_dict()\n",
        "    \n",
        "    metrics = {\n",
        "        'in_sample': insample_metrics,\n",
        "        'cv_mean': cv_metrics_avg,\n",
        "        'cv_std': cv_metrics_std\n",
        "    }\n",
        "    \n",
        "    return model, scaler, X_full, metrics\n",
        "\n",
        "# Train final model\n",
        "model, scaler, X_transformed, metrics = train_final_model(\n",
        "    X_media, X_control, y, channels, best_params, cv_splits, config\n",
        ")\n",
        "\n",
        "# Display metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nIn-Sample (Full Data):\")\n",
        "for metric, value in metrics['in_sample'].items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nCross-Validation (Out-of-Sample):\")\n",
        "for metric in ['R2', 'MAPE', 'NRMSE']:\n",
        "    mean_val = metrics['cv_mean'].get(metric, 0)\n",
        "    std_val = metrics['cv_std'].get(metric, 0)\n",
        "    print(f\"  {metric}: {mean_val:.4f} ± {std_val:.4f}\")\n",
        "\n",
        "# Model quality assessment\n",
        "cv_mape = metrics['cv_mean'].get('MAPE', 100)\n",
        "if cv_mape < 10:\n",
        "    quality = \"EXCELLENT\"\n",
        "elif cv_mape < 20:\n",
        "    quality = \"GOOD\"\n",
        "elif cv_mape < 30:\n",
        "    quality = \"ACCEPTABLE\"\n",
        "else:\n",
        "    quality = \"NEEDS IMPROVEMENT\"\n",
        "\n",
        "print(f\"\\nModel Quality: {quality} (CV MAPE = {cv_mape:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "bootstrap_roi_confidence_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 9: Bootstrap Confidence Intervals for ROI\n",
        "# =============================================================================\n",
        "#\n",
        "# WHY BOOTSTRAP INSTEAD OF ANALYTIC CONFIDENCE INTERVALS:\n",
        "#\n",
        "# Traditional CI formulas assume:\n",
        "#   - Normally distributed errors\n",
        "#   - Independent observations\n",
        "#   - Linear relationships\n",
        "#\n",
        "# MMM violates all three:\n",
        "#   - Errors are often heteroscedastic (bigger in Q4)\n",
        "#   - Time series has autocorrelation (this week's revenue predicts next week's)\n",
        "#   - We applied nonlinear transforms (adstock, saturation)\n",
        "#\n",
        "# BOOTSTRAP APPROACH:\n",
        "#   1. Resample the data WITH REPLACEMENT (some weeks appear twice, some not at all)\n",
        "#   2. Re-fit the model on this \"fake\" dataset\n",
        "#   3. Calculate ROI for each channel\n",
        "#   4. Repeat 100 times\n",
        "#   5. The 5th and 95th percentiles of these 100 ROIs = 90% confidence interval\n",
        "#\n",
        "# INTERPRETATION:\n",
        "#   - ROI = 3.2 [2.8, 3.6] means: \"We estimate LinkedIn returns $3.20 per dollar,\n",
        "#     and we're 90% confident the true value is between $2.80 and $3.60\"\n",
        "#   - IS_SIGNIFICANT = True means the entire CI is above zero (we're confident\n",
        "#     the channel has positive ROI, not just statistical noise)\n",
        "#\n",
        "# NOTE: We keep adstock/saturation params FIXED during bootstrap. We're quantifying\n",
        "# uncertainty in the COEFFICIENTS, not the hyperparameters. Full uncertainty would\n",
        "# require nested optimization (computationally prohibitive).\n",
        "#\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# DEEPER DIVE: THE BOOTSTRAP PRINCIPLE\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "#\n",
        "# The key insight (Efron, 1979): The empirical distribution of your sample\n",
        "# approximates the true population distribution. By resampling FROM your sample,\n",
        "# you simulate what WOULD happen if you could re-run the entire data collection.\n",
        "#\n",
        "# For a statistic θ̂ (like ROI), the bootstrap distribution of θ̂* approximates\n",
        "# the sampling distribution of θ̂. The standard error of θ̂* over B bootstrap\n",
        "# samples estimates the true standard error of θ̂.\n",
        "#\n",
        "# PERCENTILE METHOD (what we use):\n",
        "# The (α/2, 1-α/2) percentiles of the bootstrap distribution give a (1-α) CI.\n",
        "# For 90% CI: we take the 5th and 95th percentiles of 100 bootstrap ROIs.\n",
        "#\n",
        "# ALTERNATIVE METHODS (not used here, but worth knowing):\n",
        "# - BCa (Bias-Corrected Accelerated): Adjusts for skewness and bias in θ̂\n",
        "# - Studentized Bootstrap: Divides by bootstrap SE, more accurate for small n\n",
        "# - Block Bootstrap: For time series—resamples contiguous blocks to preserve\n",
        "#   autocorrelation. We don't use this because our primary goal is coefficient\n",
        "#   uncertainty, and time-series structure is less critical for that.\n",
        "#\n",
        "# WHY 100 ITERATIONS:\n",
        "# - SE of a percentile estimate ≈ √(p(1-p)/B) where p is the percentile\n",
        "# - For p=0.05 and B=100: SE ≈ 0.022 (good enough for practical decisions)\n",
        "# - B=1000 would give SE ≈ 0.007 (diminishing returns for 10x compute)\n",
        "#\n",
        "# COEFFICIENT UNSCALING:\n",
        "# Note: We divide by scaler.scale_ to convert back to original units.\n",
        "# StandardScaler transforms: X_scaled = (X - μ) / σ\n",
        "# Ridge fits: y = Σ β_scaled[i] · X_scaled[i]\n",
        "# To get interpretable coefficients: β_original[i] = β_scaled[i] / σ[i]\n",
        "# =============================================================================\n",
        "\n",
        "def bootstrap_roi_confidence(X_media, X_control, y, channels, params, config):\n",
        "    \"\"\"\n",
        "    Bootstrap confidence intervals for channel ROI estimates.\n",
        "    \n",
        "    Resamples data 100 times, re-fits model each time, collects distribution\n",
        "    of ROI estimates. This captures uncertainty from:\n",
        "      - Sample variability (different weeks have different patterns)\n",
        "      - Coefficient estimation (regression has standard errors)\n",
        "    \n",
        "    Does NOT capture uncertainty in hyperparameters (theta, alpha, gamma).\n",
        "    \"\"\"\n",
        "    n_samples = len(y)\n",
        "    n_bootstrap = config.n_bootstrap\n",
        "    ci_level = config.confidence_level\n",
        "    \n",
        "    # Apply transformations once (params are fixed)\n",
        "    X_media_trans = apply_media_transformations(X_media, params, channels)\n",
        "    X_full = pd.concat([X_media_trans, X_control], axis=1)\n",
        "    \n",
        "    roi_samples = {ch: [] for ch in channels}\n",
        "    coef_samples = {ch: [] for ch in channels}\n",
        "    \n",
        "    print(f\"\\nRunning {n_bootstrap} bootstrap iterations for {ci_level*100:.0f}% CI...\")\n",
        "    \n",
        "    for b in range(n_bootstrap):\n",
        "        boot_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "        X_boot = X_full.iloc[boot_idx]\n",
        "        y_boot = y.iloc[boot_idx]\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_boot)\n",
        "        \n",
        "        model = Ridge(alpha=config.ridge_alpha)\n",
        "        model.fit(X_scaled, y_boot)\n",
        "        \n",
        "        coefs = model.coef_ / scaler.scale_\n",
        "        \n",
        "        for i, ch in enumerate(channels):\n",
        "            coef = coefs[i]\n",
        "            coef_samples[ch].append(coef)\n",
        "            \n",
        "            contribution = X_media_trans[ch].iloc[boot_idx].sum() * coef\n",
        "            spend = X_media[ch].iloc[boot_idx].sum()\n",
        "            roi = contribution / spend if spend > 0 else 0\n",
        "            roi_samples[ch].append(roi)\n",
        "        \n",
        "        if (b + 1) % 25 == 0:\n",
        "            print(f\"  Completed {b + 1}/{n_bootstrap}\")\n",
        "    \n",
        "    # Calculate confidence intervals\n",
        "    alpha = 1 - ci_level\n",
        "    results = []\n",
        "    \n",
        "    for ch in channels:\n",
        "        rois = np.array(roi_samples[ch])\n",
        "        coefs = np.array(coef_samples[ch])\n",
        "        ci_key = f'ROI_CI_{int(ci_level*100)}'\n",
        "        \n",
        "        results.append({\n",
        "            'CHANNEL_KEY': ch,\n",
        "            'ROI_MEAN': np.mean(rois),\n",
        "            'ROI_MEDIAN': np.median(rois),\n",
        "            'ROI_STD': np.std(rois),\n",
        "            f'ROI_CI_LOWER_{int(ci_level*100)}': np.percentile(rois, alpha/2 * 100),\n",
        "            f'ROI_CI_UPPER_{int(ci_level*100)}': np.percentile(rois, (1 - alpha/2) * 100),\n",
        "            'COEF_MEAN': np.mean(coefs),\n",
        "            'COEF_STD': np.std(coefs),\n",
        "            'TOTAL_SPEND': X_media[ch].sum()\n",
        "        })\n",
        "    \n",
        "    roi_ci = pd.DataFrame(results)\n",
        "    roi_ci['IS_SIGNIFICANT'] = roi_ci[f'ROI_CI_LOWER_{int(ci_level*100)}'] > 0\n",
        "    \n",
        "    return roi_ci.sort_values('ROI_MEAN', ascending=False)\n",
        "\n",
        "# Run bootstrap\n",
        "roi_confidence = bootstrap_roi_confidence(\n",
        "    X_media, X_control, y, channels, best_params, config\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CHANNEL ROI WITH CONFIDENCE INTERVALS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nConfidence Level: {config.confidence_level*100:.0f}%\")\n",
        "print(\"\\nTop 10 Channels by ROI:\")\n",
        "display_cols = ['CHANNEL_KEY', 'ROI_MEAN', f'ROI_CI_LOWER_{int(config.confidence_level*100)}', \n",
        "                f'ROI_CI_UPPER_{int(config.confidence_level*100)}', 'IS_SIGNIFICANT', 'TOTAL_SPEND']\n",
        "print(roi_confidence[display_cols].head(10).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "generate_response_curves_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 10: Generate Response Curves and Marginal ROI\n",
        "# =============================================================================\n",
        "#\n",
        "# RESPONSE CURVES: THE KEY VISUALIZATION\n",
        "#\n",
        "# A response curve shows predicted revenue contribution as a function of spend:\n",
        "#\n",
        "#   Revenue │              ●●●●●●●●●●●●  ← Saturation zone (mROI < 1)\n",
        "#   Contrib │         ●●●●●               Each extra $ returns < $1\n",
        "#     ▲     │      ●●●                    = \"Wasted spend\"\n",
        "#     │     │    ●●\n",
        "#     │     │  ●●                       ← Efficient zone (mROI > 1)\n",
        "#     │     │●●                           Each extra $ returns > $1\n",
        "#     └─────┴──────────────────────────► Spend\n",
        "#           0      $50k     $100k\n",
        "#\n",
        "# AVERAGE ROI vs. MARGINAL ROI:\n",
        "#\n",
        "#   Average ROI = Total contribution / Total spend\n",
        "#               = \"What did we get back per dollar HISTORICALLY?\"\n",
        "#\n",
        "#   Marginal ROI = d(contribution)/d(spend) at CURRENT spend level\n",
        "#                = \"What will we get back for the NEXT dollar?\"\n",
        "#\n",
        "# MARGINAL ROI IS MORE USEFUL because:\n",
        "#   - A channel with 5x average ROI might be saturated (0.5x marginal)\n",
        "#   - A channel with 2x average ROI might have headroom (3x marginal)\n",
        "#   - Budget decisions are about the NEXT dollar, not past dollars\n",
        "#\n",
        "# HOW WE CALCULATE MARGINAL ROI:\n",
        "#   Numerical derivative: [f(x + δ) - f(x)] / δ\n",
        "#   where f(x) = coefficient × hill_saturation(adstock(x))\n",
        "#\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# DEEPER DIVE: THE CALCULUS OF MARGINAL ROI\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "#\n",
        "# The full response function is a composition of three transforms:\n",
        "#\n",
        "#   R(s) = β · H(A(s))\n",
        "#\n",
        "# where:\n",
        "#   s = weekly spend\n",
        "#   A(s) = s/(1-θ) = steady-state adstock (geometric series limit)\n",
        "#   H(x) = x^α / (x^α + γ^α) = Hill saturation function\n",
        "#   β = regression coefficient (revenue per unit of saturated adstock)\n",
        "#\n",
        "# The marginal ROI is dR/ds, applying chain rule:\n",
        "#\n",
        "#   dR/ds = β · dH/dA · dA/ds\n",
        "#\n",
        "#   dA/ds = 1/(1-θ)  (linear relationship at steady state)\n",
        "#\n",
        "#   dH/dA = α·γ^α·A^(α-1) / (A^α + γ^α)²  (Hill function derivative)\n",
        "#\n",
        "# Substituting:\n",
        "#   mROI = β · [α·γ^α·A^(α-1) / (A^α + γ^α)²] · [1/(1-θ)]\n",
        "#\n",
        "# KEY INSIGHT: As A → ∞ (high spend), (A^α + γ^α)² grows as A^(2α), but the\n",
        "# numerator only grows as A^(α-1). Net effect: mROI → 0 as spend → ∞.\n",
        "# This is diminishing returns made explicit through calculus.\n",
        "#\n",
        "# AT THE HALF-SATURATION POINT (A = γ):\n",
        "#   H(γ) = 0.5, and dH/dA = α/(4γ)\n",
        "#   mROI = β · α/(4γ) · 1/(1-θ)\n",
        "#\n",
        "# This gives us a quick diagnostic: if mROI at current spend is close to\n",
        "# α·β/(4γ(1-θ)), we're roughly at the \"efficient frontier\" of the S-curve.\n",
        "#\n",
        "# NUMERICAL VS. ANALYTIC DERIVATIVE:\n",
        "# We use numerical differentiation [f(x+δ) - f(x)]/δ for simplicity and to\n",
        "# match what the optimizer actually \"sees\". Analytic form above is for intuition.\n",
        "#\n",
        "# EFFICIENCY ZONE THRESHOLDS:\n",
        "#   mROI > 1.5: EFFICIENT - Every dollar returns >$1.50, strong investment\n",
        "#   mROI 0.8-1.5: DIMINISHING - Still positive but flattening\n",
        "#   mROI < 0.8: SATURATED - Likely better to reallocate to other channels\n",
        "#\n",
        "# These thresholds are heuristics, not physical laws. Adjust based on your\n",
        "# cost of capital and strategic priorities.\n",
        "# =============================================================================\n",
        "\n",
        "def generate_response_curves(X_media, channels, params, coefficients, roi_confidence, n_points=100):\n",
        "    \"\"\"\n",
        "    Generate response curves with confidence intervals and efficiency zones.\n",
        "    \n",
        "    Response curves show the spend → revenue relationship for each channel.\n",
        "    Marginal ROI is the slope of this curve at current spend level.\n",
        "    \n",
        "    ENHANCED OUTPUT INCLUDES:\n",
        "    - CI bands: Upper/lower predictions based on bootstrap coefficient variance\n",
        "    - Marginal ROI at each point: Answers \"what's the next dollar worth HERE?\"\n",
        "    - Efficiency zone: EFFICIENT (mROI > 1.5), DIMINISHING (0.8-1.5), SATURATED (< 0.8)\n",
        "    \"\"\"\n",
        "    curves = []\n",
        "    marginal_roi = {}\n",
        "    \n",
        "    # Re-fit to get current coefficients\n",
        "    X_media_trans = apply_media_transformations(X_media, params, channels)\n",
        "    X_full = pd.concat([X_media_trans, X_control], axis=1)\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_full)\n",
        "    model.fit(X_scaled, y)\n",
        "    coefficients = dict(zip(channels, model.coef_[:len(channels)] / scaler.scale_[:len(channels)]))\n",
        "    \n",
        "    # Get coefficient uncertainty from bootstrap (for CI bands)\n",
        "    coef_std = {}\n",
        "    for ch in channels:\n",
        "        if roi_confidence is not None and 'COEF_STD' in roi_confidence.columns:\n",
        "            ch_row = roi_confidence[roi_confidence['CHANNEL_KEY'] == ch]\n",
        "            coef_std[ch] = ch_row['COEF_STD'].values[0] if len(ch_row) > 0 else 0\n",
        "        else:\n",
        "            coef_std[ch] = coefficients.get(ch, 0) * 0.15  # Default 15% uncertainty\n",
        "    \n",
        "    for ch in channels:\n",
        "        p = params[ch]\n",
        "        coef = coefficients.get(ch, 0)\n",
        "        coef_uncertainty = coef_std.get(ch, 0)\n",
        "        \n",
        "        current_spend = X_media[ch].mean()\n",
        "        max_spend = X_media[ch].max() * 3\n",
        "        gamma = p['gamma']  # Half-saturation point\n",
        "        \n",
        "        spend_range = np.linspace(0, max_spend, n_points)\n",
        "        \n",
        "        for i, spend in enumerate(spend_range):\n",
        "            adstock_steady = spend / (1 - p['theta']) if p['theta'] < 1 else spend\n",
        "            saturated = hill_saturation(np.array([adstock_steady]), p['alpha'], p['gamma'])[0]\n",
        "            contribution = saturated * coef\n",
        "            \n",
        "            # CI bands (scale coefficient uncertainty through saturation transform)\n",
        "            contribution_ci_lower = saturated * max(0, coef - 1.645 * coef_uncertainty)\n",
        "            contribution_ci_upper = saturated * (coef + 1.645 * coef_uncertainty)\n",
        "            \n",
        "            # Marginal ROI at this spend level (numerical derivative)\n",
        "            delta = max(spend * 0.01, 100)  # At least $100 increment\n",
        "            adstock_delta = (spend + delta) / (1 - p['theta']) if p['theta'] < 1 else (spend + delta)\n",
        "            response_delta = hill_saturation(np.array([adstock_delta]), p['alpha'], p['gamma'])[0] * coef\n",
        "            marginal_roi_at_spend = (response_delta - contribution) / delta if delta > 0 else 0\n",
        "            \n",
        "            # Classify efficiency zone based on marginal ROI\n",
        "            if marginal_roi_at_spend > 1.5:\n",
        "                zone = 'EFFICIENT'\n",
        "            elif marginal_roi_at_spend >= 0.8:\n",
        "                zone = 'DIMINISHING'\n",
        "            else:\n",
        "                zone = 'SATURATED'\n",
        "            \n",
        "            curves.append({\n",
        "                'CHANNEL': ch,\n",
        "                'SPEND': spend,\n",
        "                'PREDICTED_REVENUE': contribution,\n",
        "                'PREDICTED_REVENUE_CI_LOWER': contribution_ci_lower,\n",
        "                'PREDICTED_REVENUE_CI_UPPER': contribution_ci_upper,\n",
        "                'MARGINAL_ROI_AT_SPEND': marginal_roi_at_spend,\n",
        "                'EFFICIENCY_ZONE': zone\n",
        "            })\n",
        "        \n",
        "        # Marginal ROI at current spend (for summary)\n",
        "        delta = current_spend * 0.01\n",
        "        adstock_curr = current_spend / (1 - p['theta']) if p['theta'] < 1 else current_spend\n",
        "        response_curr = hill_saturation(np.array([adstock_curr]), p['alpha'], p['gamma'])[0] * coef\n",
        "        \n",
        "        adstock_delta = (current_spend + delta) / (1 - p['theta']) if p['theta'] < 1 else (current_spend + delta)\n",
        "        response_delta = hill_saturation(np.array([adstock_delta]), p['alpha'], p['gamma'])[0] * coef\n",
        "        \n",
        "        marginal_roi[ch] = (response_delta - response_curr) / delta if delta > 0 else 0\n",
        "    \n",
        "    return pd.DataFrame(curves), marginal_roi\n",
        "\n",
        "# Generate curves with CI bands\n",
        "response_curves, marginal_roi = generate_response_curves(X_media, channels, best_params, {}, roi_confidence)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MARGINAL ROI (Value of Next Dollar Spent)\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nTop 10 Channels by Marginal ROI:\")\n",
        "marginal_df = pd.DataFrame([\n",
        "    {'CHANNEL_KEY': ch, 'MARGINAL_ROI': roi} \n",
        "    for ch, roi in marginal_roi.items()\n",
        "]).sort_values('MARGINAL_ROI', ascending=False)\n",
        "print(marginal_df.head(10).to_string(index=False))\n",
        "print(f\"\\nResponse curves generated: {len(response_curves)} data points\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "optimize_budget_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 11: Budget Optimizer\n",
        "# =============================================================================\n",
        "#\n",
        "# THE BUDGET ALLOCATION PROBLEM:\n",
        "#\n",
        "# Given the learned response curves, how should we reallocate spend to maximize\n",
        "# total revenue? This is a constrained optimization problem:\n",
        "#\n",
        "#   MAXIMIZE: Total predicted revenue = Σ (saturated_response × coefficient)\n",
        "#   SUBJECT TO:\n",
        "#     1. Total budget unchanged (budget neutral)\n",
        "#     2. Each channel can only change ±30% (realistic for CMO approval)\n",
        "#     3. All spend >= 0 (can't have negative spend)\n",
        "#\n",
        "# WHY CONSTRAINTS MATTER:\n",
        "#\n",
        "# Without constraints, the optimizer would say \"put 100% in LinkedIn\" because\n",
        "# it has the highest marginal ROI. But this is impractical:\n",
        "#   - CMOs can't pivot all spend in one quarter\n",
        "#   - Vendor contracts require minimum commitments\n",
        "#   - Channel capacity limits exist (LinkedIn inventory is finite)\n",
        "#\n",
        "# The ±30% limit keeps recommendations actionable. If LinkedIn is at $1M/quarter,\n",
        "# we recommend up to $1.3M, not $10M.\n",
        "#\n",
        "# SLSQP ALGORITHM:\n",
        "# Sequential Least Squares Programming - a constrained optimizer that handles\n",
        "# both equality constraints (total budget) and inequality constraints (±30%).\n",
        "# Faster than evolutionary methods for smooth, convex problems like this.\n",
        "#\n",
        "# PREDICTED LIFT:\n",
        "# The \"predicted lift\" is the difference between:\n",
        "#   - Current revenue (with current allocation)\n",
        "#   - Optimized revenue (with recommended allocation)\n",
        "# This is the \"headline number\" for the CMO: \"Shifting spend could add $2.4M\"\n",
        "#\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# DEEPER DIVE: CONSTRAINED OPTIMIZATION FORMULATION\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "#\n",
        "# We work in PROPORTION space (x_i = spend_i / total_budget) for numerical\n",
        "# stability and cleaner constraint expression.\n",
        "#\n",
        "# FORMAL PROBLEM:\n",
        "#   min  -Σ f_i(x_i · B)      (we negate because scipy.minimize minimizes)\n",
        "#   s.t. Σ x_i = 1            (equality: budget-neutral)\n",
        "#        (1-δ)·x_i^0 ≤ x_i ≤ (1+δ)·x_i^0  for all i  (inequality: ±δ bounds)\n",
        "#        x_i ≥ 0              for all i  (implicit in bounds)\n",
        "#\n",
        "# where:\n",
        "#   f_i(s) = β_i · H_i(A_i(s))  is the response function for channel i\n",
        "#   B = total budget\n",
        "#   x_i^0 = current proportion for channel i\n",
        "#   δ = 0.30 (our 30% change limit)\n",
        "#\n",
        "# WHY SLSQP (Sequential Least Squares Quadratic Programming):\n",
        "# 1. Handles both equality (budget) and inequality (bounds) constraints\n",
        "# 2. Uses quadratic approximation of the Lagrangian at each step\n",
        "# 3. Convergence is typically fast for smooth, moderately-sized problems\n",
        "# 4. Available in scipy.optimize.minimize with method='SLSQP'\n",
        "#\n",
        "# The Lagrangian for our problem:\n",
        "#   L(x, λ, μ) = -Σ f_i(x_i·B) + λ·(Σx_i - 1) + Σ μ_i·(constraint violations)\n",
        "#\n",
        "# At the optimum, KKT conditions require:\n",
        "#   ∂f_i/∂x_i = λ for all active channels (marginal returns equalized!)\n",
        "#\n",
        "# ECONOMIC INTERPRETATION:\n",
        "# The optimal allocation EQUALIZES MARGINAL ROI across all channels (subject\n",
        "# to constraints). This is the principle of marginal analysis: reallocate from\n",
        "# low-mROI to high-mROI channels until they're equal.\n",
        "#\n",
        "# If channel A has mROI = 3.0 and channel B has mROI = 1.5, we should shift\n",
        "# budget A→B until they converge (typically around 2.0 for both).\n",
        "#\n",
        "# The ±30% constraint prevents extreme shifts, so post-optimization mROIs\n",
        "# won't be perfectly equal—but they'll be closer than the starting point.\n",
        "#\n",
        "# CAUTION ON \"PREDICTED LIFT\":\n",
        "# The lift estimate assumes the model is correctly specified and extrapolates\n",
        "# well. In practice, treat it as directional. A \"10% lift\" doesn't mean you'll\n",
        "# get exactly 10%—it means reallocation is likely beneficial.\n",
        "# =============================================================================\n",
        "\n",
        "def optimize_budget(X_media, channels, params, marginal_roi, budget_change_limit=0.30):\n",
        "    \"\"\"\n",
        "    Optimize budget allocation to maximize predicted revenue.\n",
        "    \n",
        "    Uses constrained optimization (SLSQP) to find the best reallocation\n",
        "    within business constraints (±30% per channel, budget neutral).\n",
        "    \"\"\"\n",
        "    current_spend = {ch: X_media[ch].sum() for ch in channels}\n",
        "    total_budget = sum(current_spend.values())\n",
        "    \n",
        "    # Only optimize channels with actual spend (avoid divide-by-zero)\n",
        "    active_channels = [ch for ch in channels if current_spend[ch] > 0]\n",
        "    n_channels = len(active_channels)\n",
        "    \n",
        "    if n_channels == 0:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Re-fit model to get coefficients (needed for revenue prediction)\n",
        "    X_media_trans = apply_media_transformations(X_media, params, channels)\n",
        "    X_full = pd.concat([X_media_trans, X_control], axis=1)\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_full)\n",
        "    model.fit(X_scaled, y)\n",
        "    # Unscale coefficients to get \"per-unit\" impact\n",
        "    coefficients = dict(zip(channels, model.coef_[:len(channels)] / scaler.scale_[:len(channels)]))\n",
        "    \n",
        "    x0 = np.array([current_spend[ch] / total_budget for ch in active_channels])\n",
        "    \n",
        "    def objective(x):\n",
        "        total_contrib = 0\n",
        "        for i, ch in enumerate(active_channels):\n",
        "            spend = x[i] * total_budget\n",
        "            p = params[ch]\n",
        "            coef = coefficients.get(ch, 0)\n",
        "            \n",
        "            weekly_spend = spend / len(X_media)\n",
        "            adstock = weekly_spend / (1 - p['theta']) if p['theta'] < 1 else weekly_spend\n",
        "            saturated = hill_saturation(np.array([adstock]), p['alpha'], p['gamma'])[0]\n",
        "            contribution = saturated * coef * len(X_media)\n",
        "            total_contrib += contribution\n",
        "        \n",
        "        return -total_contrib\n",
        "    \n",
        "    budget_constraint = {'type': 'eq', 'fun': lambda x: np.sum(x) - 1.0}\n",
        "    \n",
        "    bounds = []\n",
        "    for i, ch in enumerate(active_channels):\n",
        "        current_prop = x0[i]\n",
        "        lower = max(0, current_prop * (1 - budget_change_limit))\n",
        "        upper = current_prop * (1 + budget_change_limit)\n",
        "        bounds.append((lower, upper))\n",
        "    \n",
        "    result = minimize(objective, x0, method='SLSQP', bounds=bounds, \n",
        "                     constraints=budget_constraint, options={'maxiter': 1000})\n",
        "    \n",
        "    results = []\n",
        "    for i, ch in enumerate(active_channels):\n",
        "        current = current_spend[ch]\n",
        "        recommended = result.x[i] * total_budget\n",
        "        change = recommended - current\n",
        "        change_pct = change / current * 100 if current > 0 else 0\n",
        "        \n",
        "        results.append({\n",
        "            'CHANNEL_KEY': ch,\n",
        "            'CURRENT_SPEND': current,\n",
        "            'RECOMMENDED_SPEND': recommended,\n",
        "            'CHANGE_AMOUNT': change,\n",
        "            'CHANGE_PCT': change_pct,\n",
        "            'MARGINAL_ROI': marginal_roi.get(ch, 0)\n",
        "        })\n",
        "    \n",
        "    opt_df = pd.DataFrame(results).sort_values('CHANGE_PCT', ascending=False)\n",
        "    \n",
        "    current_contribution = -objective(x0)\n",
        "    optimized_contribution = -objective(result.x)\n",
        "    predicted_lift = optimized_contribution - current_contribution\n",
        "    \n",
        "    print(f\"\\nPredicted Revenue Lift: ${predicted_lift:,.0f}\")\n",
        "    print(f\"Lift Percentage: {predicted_lift / current_contribution * 100:.1f}%\")\n",
        "    \n",
        "    return opt_df\n",
        "\n",
        "# Run budget optimization\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BUDGET OPTIMIZATION RECOMMENDATIONS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nConstraints: Budget neutral, ±{config.budget_change_limit*100:.0f}% per channel\")\n",
        "\n",
        "budget_recommendations = optimize_budget(\n",
        "    X_media, channels, best_params, marginal_roi, config.budget_change_limit\n",
        ")\n",
        "\n",
        "print(\"\\nTop 5 Channels to INCREASE:\")\n",
        "print(budget_recommendations.head(5)[['CHANNEL_KEY', 'CURRENT_SPEND', 'RECOMMENDED_SPEND', 'CHANGE_PCT']].to_string(index=False))\n",
        "\n",
        "print(\"\\nTop 5 Channels to DECREASE:\")\n",
        "print(budget_recommendations.tail(5)[['CHANNEL_KEY', 'CURRENT_SPEND', 'RECOMMENDED_SPEND', 'CHANGE_PCT']].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "prepare_model_results_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 12: Prepare Results with Dimensional Keys\n",
        "# =============================================================================\n",
        "#\n",
        "# OUTPUT STRUCTURE:\n",
        "#\n",
        "# The notebook produces granular ROI at the Channel × Region × Product level.\n",
        "# Example: \"LINKEDIN_EMEA_SI\" → LinkedIn in EMEA for Safety & Industrial.\n",
        "#\n",
        "# We parse this back to separate columns so results can JOIN to dimension tables:\n",
        "#   - CHANNEL_CODE → joins to MARKETING_CHANNEL dimension\n",
        "#   - GEO_CODE → joins to GEOGRAPHY dimension (at configured level)\n",
        "#   - PRODUCT_CODE → joins to PRODUCT_CATEGORY dimension (at configured level)\n",
        "#\n",
        "# This enables the Streamlit app to filter/slice results by any dimension\n",
        "# without string parsing in SQL.\n",
        "#\n",
        "# KEY OUTPUT COLUMNS:\n",
        "#   - ROI: Average historical return (contribution / spend)\n",
        "#   - ROI_CI_LOWER/UPPER: Bootstrap confidence bounds\n",
        "#   - MARGINAL_ROI: Return on NEXT dollar (derivative of response curve)\n",
        "#   - OPTIMAL_SPEND_SUGGESTION: Budget optimizer recommendation\n",
        "#   - ADSTOCK_DECAY_RATE: Learned theta (how quickly effect fades)\n",
        "#   - SATURATION_POINT: Learned gamma (spend level at 50% of max response)\n",
        "# =============================================================================\n",
        "\n",
        "def parse_channel_key(channel_key):\n",
        "    \"\"\"\n",
        "    Parse composite channel key back to dimensions.\n",
        "    E.g., \"LINKEDIN_EMEA_SI\" → {CHANNEL: LINKEDIN, GEO: EMEA, PRODUCT: SI}\n",
        "    \"\"\"\n",
        "    parts = channel_key.split('_')\n",
        "    if len(parts) >= 3:\n",
        "        return {'CHANNEL': parts[0], 'GEO': parts[1], 'PRODUCT': parts[2]}\n",
        "    elif len(parts) == 2:\n",
        "        return {'CHANNEL': parts[0], 'GEO': parts[1], 'PRODUCT': 'ALL'}\n",
        "    else:\n",
        "        return {'CHANNEL': parts[0] if parts else 'UNKNOWN', 'GEO': 'ALL', 'PRODUCT': 'ALL'}\n",
        "\n",
        "\n",
        "def prepare_model_results(roi_confidence, marginal_roi, budget_recommendations, params, config, metrics, X_media):\n",
        "    \"\"\"\n",
        "    Prepare final results DataFrame for saving to MMM.MODEL_RESULTS.\n",
        "    \n",
        "    ENHANCED OUTPUT now includes:\n",
        "    - Full uncertainty quantification (CI bounds, significance)\n",
        "    - Learned MMM parameters (adstock decay, saturation shape/scale)\n",
        "    - Model quality metrics (R², CV MAPE)\n",
        "    - Spend context (current spend, share of budget)\n",
        "    \"\"\"\n",
        "    \n",
        "    results = []\n",
        "    ci_level = int(config.confidence_level * 100)\n",
        "    \n",
        "    # Calculate total spend across all channels for share calculation\n",
        "    total_spend_all = sum(row['TOTAL_SPEND'] for _, row in roi_confidence.iterrows())\n",
        "    \n",
        "    for _, row in roi_confidence.iterrows():\n",
        "        ch = row['CHANNEL_KEY']\n",
        "        dims = parse_channel_key(ch)\n",
        "        p = params.get(ch, {'theta': 0, 'alpha': 1, 'gamma': 1})\n",
        "        \n",
        "        budget_row = budget_recommendations[budget_recommendations['CHANNEL_KEY'] == ch]\n",
        "        optimal_spend = budget_row['RECOMMENDED_SPEND'].values[0] if len(budget_row) > 0 else row['TOTAL_SPEND']\n",
        "        \n",
        "        # Calculate spend share\n",
        "        spend_share = row['TOTAL_SPEND'] / total_spend_all if total_spend_all > 0 else 0\n",
        "        \n",
        "        # Count observations for this channel\n",
        "        n_obs = len(X_media[ch].dropna()) if ch in X_media.columns else 0\n",
        "        \n",
        "        results.append({\n",
        "            # Identifiers\n",
        "            'MODEL_RUN_DATE': datetime.now().strftime('%Y-%m-%d'),\n",
        "            'MODEL_VERSION': config.model_version,\n",
        "            'CHANNEL_CODE': dims['CHANNEL'],\n",
        "            'GEO_CODE': dims['GEO'],\n",
        "            'PRODUCT_CODE': dims['PRODUCT'],\n",
        "            'CHANNEL_KEY': ch,\n",
        "            \n",
        "            # Core metrics\n",
        "            'COEFFICIENT_WEIGHT': row['COEF_MEAN'],\n",
        "            'ROI': row['ROI_MEAN'],\n",
        "            'MARGINAL_ROI': marginal_roi.get(ch, 0),\n",
        "            \n",
        "            # Confidence intervals (90% CI from bootstrap)\n",
        "            'ROI_CI_LOWER': row[f'ROI_CI_LOWER_{ci_level}'],\n",
        "            'ROI_CI_UPPER': row[f'ROI_CI_UPPER_{ci_level}'],\n",
        "            'IS_SIGNIFICANT': row['IS_SIGNIFICANT'],\n",
        "            \n",
        "            # Learned MMM parameters\n",
        "            'ADSTOCK_DECAY_RATE': p['theta'],\n",
        "            'SATURATION_ALPHA': p['alpha'],  # Hill shape parameter\n",
        "            'SATURATION_POINT': p['gamma'],   # Half-saturation spend level\n",
        "            \n",
        "            # Model quality\n",
        "            'MODEL_R2_INSAMPLE': metrics['in_sample']['R2'],\n",
        "            'MODEL_MAPE_CV': metrics['cv_mean'].get('MAPE', None),\n",
        "            'N_OBSERVATIONS': n_obs,\n",
        "            \n",
        "            # Spend context\n",
        "            'CURRENT_SPEND': row['TOTAL_SPEND'],\n",
        "            'SPEND_SHARE': spend_share,\n",
        "            'OPTIMAL_SPEND_SUGGESTION': optimal_spend\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Prepare results with enhanced fields\n",
        "model_results = prepare_model_results(\n",
        "    roi_confidence, marginal_roi, budget_recommendations, best_params, config, metrics, X_media\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTotal records: {len(model_results)}\")\n",
        "print(f\"Model version: {config.model_version}\")\n",
        "print(f\"\\nColumns: {model_results.columns.tolist()}\")\n",
        "model_results.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "roi_analysis_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 13b: ROI Analysis with Spend Context\n",
        "# =============================================================================\n",
        "#\n",
        "# KEY INSIGHT: ROI estimates are only reliable for channels with sufficient spend.\n",
        "# Low-spend channels have inflated/uncertain ROI due to signal-to-noise issues.\n",
        "#\n",
        "# This cell provides:\n",
        "# 1. ROI by Channel (all channels) - shows raw model output with spend labels\n",
        "# 2. ROI vs Spend scatter - reveals the spend-reliability relationship\n",
        "# 3. Filtered ROI view - user can exclude low-spend channels via slider\n",
        "#\n",
        "# The story: \"High-spend channels have reliable ROI estimates. Low-spend channels\n",
        "# need additional validation through incrementality testing.\"\n",
        "# =============================================================================\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "# Color palette for consistent styling (matches Streamlit app)\n",
        "COLORS = {\n",
        "    'primary': '#0068C9',      # Snowflake blue\n",
        "    'secondary': '#11567F',    # Deep blue  \n",
        "    'positive': '#28A745',     # Green for positive metrics\n",
        "    'negative': '#DC3545',     # Red for negative metrics\n",
        "    'neutral': '#6B7280',      # Medium gray\n",
        "    'accent': '#D95F02',       # Orange highlight\n",
        "    'warning': '#F39C12'       # Amber for caution\n",
        "}\n",
        "\n",
        "ci_level = int(config.confidence_level * 100)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# PANEL 1: ROI by Channel (All Channels) with Spend Labels\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "st.subheader(\"1. ROI by Channel (All Channels)\")\n",
        "st.caption(\"⚠️ Note: Low-spend channels may show inflated ROI due to limited signal\")\n",
        "\n",
        "roi_chart_data = roi_confidence.sort_values('ROI_MEAN', ascending=True).tail(15)\n",
        "\n",
        "# Add spend info to labels for context\n",
        "roi_chart_data = roi_chart_data.copy()\n",
        "roi_chart_data['LABEL'] = roi_chart_data.apply(\n",
        "    lambda r: f\"{r['CHANNEL_KEY']} (${r['TOTAL_SPEND']/1e6:.0f}M)\", axis=1\n",
        ")\n",
        "\n",
        "fig_roi = go.Figure()\n",
        "\n",
        "fig_roi.add_trace(go.Bar(\n",
        "    y=roi_chart_data['LABEL'],\n",
        "    x=roi_chart_data['ROI_MEAN'],\n",
        "    orientation='h',\n",
        "    marker_color=[COLORS['positive'] if sig else COLORS['neutral'] \n",
        "                  for sig in roi_chart_data['IS_SIGNIFICANT']],\n",
        "    error_x=dict(\n",
        "        type='data',\n",
        "        symmetric=False,\n",
        "        array=roi_chart_data[f'ROI_CI_UPPER_{ci_level}'] - roi_chart_data['ROI_MEAN'],\n",
        "        arrayminus=roi_chart_data['ROI_MEAN'] - roi_chart_data[f'ROI_CI_LOWER_{ci_level}'],\n",
        "        color='rgba(0,0,0,0.3)'\n",
        "    ),\n",
        "    hovertemplate='<b>%{y}</b><br>ROI: %{x:.2f}x<extra></extra>'\n",
        "))\n",
        "\n",
        "fig_roi.add_vline(x=1.0, line_dash=\"dash\", line_color=COLORS['warning'],\n",
        "                  annotation_text=\"Break-even (1.0x)\", annotation_position=\"top right\")\n",
        "\n",
        "fig_roi.update_layout(\n",
        "    title=dict(\n",
        "        text=f\"<b>Channel ROI Rankings</b><br><sup>Labels show total spend | Green = statistically significant</sup>\",\n",
        "        x=0.5, xanchor='center'\n",
        "    ),\n",
        "    xaxis_title=\"Return on Investment (ROI)\",\n",
        "    yaxis_title=\"Channel (Total Spend)\",\n",
        "    height=500,\n",
        "    showlegend=False,\n",
        "    plot_bgcolor='rgba(0,0,0,0)',\n",
        "    paper_bgcolor='rgba(0,0,0,0)',\n",
        ")\n",
        "fig_roi.update_xaxes(gridcolor='lightgray', zeroline=True, zerolinecolor='gray')\n",
        "st.plotly_chart(fig_roi, use_container_width=True)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# PANEL 2: ROI vs Spend Scatter (The Key Insight)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "st.subheader(\"2. ROI vs Total Spend\")\n",
        "st.caption(\"📊 Notice: Low-spend channels (left side) often show extreme ROI values - this is the signal reliability issue\")\n",
        "\n",
        "scatter_data = roi_confidence.copy()\n",
        "scatter_data['SPEND_MILLIONS'] = scatter_data['TOTAL_SPEND'] / 1_000_000\n",
        "\n",
        "fig_scatter_roi = go.Figure()\n",
        "\n",
        "fig_scatter_roi.add_trace(go.Scatter(\n",
        "    x=scatter_data['SPEND_MILLIONS'],\n",
        "    y=scatter_data['ROI_MEAN'],\n",
        "    mode='markers+text',\n",
        "    marker=dict(\n",
        "        size=scatter_data['SPEND_MILLIONS'] / scatter_data['SPEND_MILLIONS'].max() * 40 + 10,\n",
        "        color=[COLORS['positive'] if sig else COLORS['neutral'] for sig in scatter_data['IS_SIGNIFICANT']],\n",
        "        line=dict(width=1, color='white')\n",
        "    ),\n",
        "    text=scatter_data['CHANNEL_KEY'].str.replace('_GLOBAL_ALL', ''),\n",
        "    textposition='top center',\n",
        "    textfont=dict(size=9),\n",
        "    hovertemplate='<b>%{text}</b><br>Spend: $%{x:.1f}M<br>ROI: %{y:.2f}x<extra></extra>'\n",
        "))\n",
        "\n",
        "fig_scatter_roi.add_hline(y=1.0, line_dash=\"dash\", line_color=COLORS['warning'],\n",
        "                          annotation_text=\"Break-even (1.0x)\")\n",
        "\n",
        "# Add reliability zones\n",
        "max_spend_val = scatter_data['SPEND_MILLIONS'].max()\n",
        "fig_scatter_roi.add_vrect(x0=0, x1=max_spend_val*0.15, fillcolor=\"rgba(255,0,0,0.1)\", \n",
        "                          line_width=0, annotation_text=\"⚠️ Low confidence\",\n",
        "                          annotation_position=\"top left\")\n",
        "\n",
        "fig_scatter_roi.update_layout(\n",
        "    title=dict(\n",
        "        text=\"<b>Spend Level vs ROI Estimate</b><br><sup>Red zone = low spend = unreliable ROI estimates</sup>\",\n",
        "        x=0.5, xanchor='center'\n",
        "    ),\n",
        "    xaxis_title=\"Total Spend ($M)\",\n",
        "    yaxis_title=\"ROI (Model Estimate)\",\n",
        "    height=500,\n",
        "    showlegend=False,\n",
        "    plot_bgcolor='rgba(0,0,0,0)',\n",
        "    paper_bgcolor='rgba(0,0,0,0)'\n",
        ")\n",
        "fig_scatter_roi.update_xaxes(gridcolor='lightgray')\n",
        "fig_scatter_roi.update_yaxes(gridcolor='lightgray')\n",
        "st.plotly_chart(fig_scatter_roi, use_container_width=True)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# PANEL 3: Filtered ROI View (Interactive)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "st.subheader(\"3. Filtered ROI View (Reliable Estimates Only)\")\n",
        "\n",
        "min_spend_val = roi_confidence['TOTAL_SPEND'].min() / 1e6\n",
        "max_spend_val = roi_confidence['TOTAL_SPEND'].max() / 1e6\n",
        "default_threshold = max(min_spend_val, max_spend_val * 0.10)\n",
        "\n",
        "spend_threshold = st.slider(\n",
        "    \"Minimum spend threshold ($M)\",\n",
        "    min_value=float(min_spend_val),\n",
        "    max_value=float(max_spend_val * 0.5),\n",
        "    value=float(default_threshold),\n",
        "    step=1.0,\n",
        "    help=\"Filter out low-spend channels with unreliable ROI estimates\"\n",
        ")\n",
        "\n",
        "filtered_data = roi_confidence[roi_confidence['TOTAL_SPEND'] >= spend_threshold * 1e6].copy()\n",
        "filtered_data = filtered_data.sort_values('ROI_MEAN', ascending=True)\n",
        "\n",
        "if len(filtered_data) > 0:\n",
        "    st.info(f\"Showing {len(filtered_data)} channels with spend ≥ ${spend_threshold:.0f}M\")\n",
        "    \n",
        "    fig_filtered = go.Figure()\n",
        "    \n",
        "    fig_filtered.add_trace(go.Bar(\n",
        "        y=filtered_data['CHANNEL_KEY'],\n",
        "        x=filtered_data['ROI_MEAN'],\n",
        "        orientation='h',\n",
        "        marker_color=[COLORS['positive'] if sig else COLORS['neutral'] \n",
        "                      for sig in filtered_data['IS_SIGNIFICANT']],\n",
        "        error_x=dict(\n",
        "            type='data',\n",
        "            symmetric=False,\n",
        "            array=filtered_data[f'ROI_CI_UPPER_{ci_level}'] - filtered_data['ROI_MEAN'],\n",
        "            arrayminus=filtered_data['ROI_MEAN'] - filtered_data[f'ROI_CI_LOWER_{ci_level}'],\n",
        "            color='rgba(0,0,0,0.3)'\n",
        "        ),\n",
        "        hovertemplate='<b>%{y}</b><br>ROI: %{x:.2f}x<extra></extra>'\n",
        "    ))\n",
        "    \n",
        "    fig_filtered.add_vline(x=1.0, line_dash=\"dash\", line_color=COLORS['warning'])\n",
        "    \n",
        "    fig_filtered.update_layout(\n",
        "        title=dict(\n",
        "            text=f\"<b>ROI Rankings (Spend ≥ ${spend_threshold:.0f}M)</b><br><sup>These estimates have sufficient signal for reliability</sup>\",\n",
        "            x=0.5, xanchor='center'\n",
        "        ),\n",
        "        xaxis_title=\"Return on Investment (ROI)\",\n",
        "        yaxis_title=\"Channel\",\n",
        "        height=400,\n",
        "        showlegend=False,\n",
        "        plot_bgcolor='rgba(0,0,0,0)',\n",
        "        paper_bgcolor='rgba(0,0,0,0)',\n",
        "    )\n",
        "    fig_filtered.update_xaxes(gridcolor='lightgray', zeroline=True, zerolinecolor='gray')\n",
        "    st.plotly_chart(fig_filtered, use_container_width=True)\n",
        "else:\n",
        "    st.warning(\"No channels meet the spend threshold. Lower the threshold to see results.\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "---\n",
        "**Key Takeaway:** The model reliably identifies ROI for high-spend channels. \n",
        "For low-spend channels showing extreme ROI values, we recommend:\n",
        "- **Geo-holdout tests** to validate performance\n",
        "- **Incrementality testing** before scaling spend\n",
        "- **Treating model estimates as directional** rather than precise\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "detailed_visualizations_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 13c: Additional Visualizations (Model Fit, Response Curves, etc.)\n",
        "# =============================================================================\n",
        "#\n",
        "# This cell contains supplementary visualizations with INTERACTIVE FILTERS:\n",
        "# - Model Fit (Actual vs Predicted)\n",
        "# - Response Curves (Diminishing Returns) - all channels selectable\n",
        "# - Channel Metrics Table (Technical Analysis)\n",
        "# - Channel Contribution Treemap\n",
        "# =============================================================================\n",
        "\n",
        "import math\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# GLOBAL FILTERS FOR THIS SECTION\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "st.markdown(\"---\")\n",
        "st.header(\"📊 Detailed Analysis\")\n",
        "\n",
        "# Prepare channel list sorted by spend\n",
        "all_channels_sorted = sorted(channels, key=lambda ch: X_media[ch].sum(), reverse=True)\n",
        "channel_display_names = {ch: ch.replace('_GLOBAL_ALL', '').replace('_', ' ') for ch in all_channels_sorted}\n",
        "\n",
        "# Top N channels by spend (for default selection)\n",
        "top_6_by_spend = all_channels_sorted[:6]\n",
        "\n",
        "# Channel selection\n",
        "st.subheader(\"Channel Selection\")\n",
        "col_filter1, col_filter2 = st.columns([3, 1])\n",
        "\n",
        "with col_filter1:\n",
        "    selected_channels = st.multiselect(\n",
        "        \"Select channels to analyze\",\n",
        "        options=all_channels_sorted,\n",
        "        default=top_6_by_spend,\n",
        "        format_func=lambda x: channel_display_names[x],\n",
        "        help=\"Choose which channels to show in response curves and metrics table\"\n",
        "    )\n",
        "\n",
        "with col_filter2:\n",
        "    if st.button(\"Select All\"):\n",
        "        selected_channels = all_channels_sorted\n",
        "    if st.button(\"Top 6 by Spend\"):\n",
        "        selected_channels = top_6_by_spend\n",
        "\n",
        "if len(selected_channels) == 0:\n",
        "    st.warning(\"Please select at least one channel to display.\")\n",
        "    selected_channels = top_6_by_spend  # Fallback\n",
        "\n",
        "st.info(f\"Analyzing {len(selected_channels)} channels\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# CHART: Model Fit - Predicted vs Actual (Aggregated Weekly)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "st.subheader(\"4. Model Fit: Predicted vs Actual Revenue\")\n",
        "\n",
        "# Get predictions from the trained model (y and y_pred are at the same level)\n",
        "X_media_trans = apply_media_transformations(X_media, best_params, channels)\n",
        "X_full_viz = pd.concat([X_media_trans, X_control], axis=1)\n",
        "X_scaled_viz = scaler.transform(X_full_viz)\n",
        "y_pred_viz = model.predict(X_scaled_viz)\n",
        "\n",
        "# y and y_pred are aligned - get corresponding weeks\n",
        "# Note: y was created from aggregated weekly data, so len(y) == number of weeks\n",
        "weeks = df.groupby('WEEK_START').first().reset_index()['WEEK_START'].values[:len(y)]\n",
        "\n",
        "weekly_fit = pd.DataFrame({\n",
        "    'WEEK_START': weeks,\n",
        "    'Actual': y.values,\n",
        "    'Predicted': y_pred_viz\n",
        "})\n",
        "\n",
        "# Calculate weekly metrics\n",
        "correlation = weekly_fit['Actual'].corr(weekly_fit['Predicted'])\n",
        "weekly_mape = np.mean(np.abs((weekly_fit['Actual'] - weekly_fit['Predicted']) / weekly_fit['Actual'])) * 100\n",
        "\n",
        "fig_fit = go.Figure()\n",
        "\n",
        "# Actual revenue\n",
        "fig_fit.add_trace(go.Scatter(\n",
        "    x=weekly_fit['WEEK_START'],\n",
        "    y=weekly_fit['Actual'],\n",
        "    mode='lines',\n",
        "    name='Actual Revenue',\n",
        "    line=dict(color=COLORS['primary'], width=2),\n",
        "    fill='tozeroy',\n",
        "    fillcolor='rgba(0, 104, 201, 0.1)'\n",
        "))\n",
        "\n",
        "# Predicted revenue\n",
        "fig_fit.add_trace(go.Scatter(\n",
        "    x=weekly_fit['WEEK_START'],\n",
        "    y=weekly_fit['Predicted'],\n",
        "    mode='lines',\n",
        "    name='Predicted Revenue',\n",
        "    line=dict(color=COLORS['accent'], width=2, dash='dash')\n",
        "))\n",
        "\n",
        "fig_fit.update_layout(\n",
        "    title=dict(\n",
        "        text=f\"<b>Model Fit: Actual vs Predicted Weekly Revenue</b><br><sup>Correlation: {correlation:.2f} | Weekly MAPE: {weekly_mape:.1f}%</sup>\",\n",
        "        x=0.5, xanchor='center'\n",
        "    ),\n",
        "    xaxis_title=\"Week\",\n",
        "    yaxis_title=\"Revenue ($)\",\n",
        "    height=450,\n",
        "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='center', x=0.5),\n",
        "    plot_bgcolor='rgba(0,0,0,0)',\n",
        "    paper_bgcolor='rgba(0,0,0,0)',\n",
        "    hovermode='x unified'\n",
        ")\n",
        "fig_fit.update_xaxes(gridcolor='lightgray')\n",
        "fig_fit.update_yaxes(gridcolor='lightgray', tickformat='$,.0f')\n",
        "st.plotly_chart(fig_fit, use_container_width=True)\n",
        "\n",
        "# Interpretation help\n",
        "if correlation > 0.7:\n",
        "    st.success(f\"✓ Good model fit - predicted revenue tracks actual revenue well (r={correlation:.2f})\")\n",
        "elif correlation > 0.4:\n",
        "    st.warning(f\"⚠ Moderate model fit - some patterns captured but significant unexplained variance (r={correlation:.2f})\")\n",
        "else:\n",
        "    st.error(f\"✗ Poor model fit - model not capturing revenue patterns (r={correlation:.2f}). Results should be interpreted with caution.\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# CHART: Response Curves (Diminishing Returns) - WEEKLY SPEND BASIS\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "st.subheader(\"5. Response Curves: Diminishing Returns\")\n",
        "st.markdown(\"\"\"\n",
        "**How to read:** X-axis is what you **spend per week** (input). Y-axis is the **revenue generated per week** (output).  \n",
        "The curve shows how additional spend produces less additional revenue (diminishing returns).  \n",
        "**Dashed orange line** = your current average weekly spend level.\n",
        "\"\"\")\n",
        "\n",
        "# Dynamic grid based on number of selected channels\n",
        "n_channels = len(selected_channels)\n",
        "n_cols = min(3, n_channels)  # Max 3 columns\n",
        "n_rows = math.ceil(n_channels / n_cols) if n_cols > 0 else 1\n",
        "\n",
        "fig_response = make_subplots(\n",
        "    rows=n_rows, cols=n_cols,\n",
        "    subplot_titles=[channel_display_names[ch] for ch in selected_channels],\n",
        "    vertical_spacing=0.12,\n",
        "    horizontal_spacing=0.08\n",
        ")\n",
        "\n",
        "for idx, ch in enumerate(selected_channels):\n",
        "    row = idx // n_cols + 1\n",
        "    col = idx % n_cols + 1\n",
        "    \n",
        "    ch_curves = response_curves[response_curves['CHANNEL'] == ch].sort_values('SPEND')\n",
        "    \n",
        "    if len(ch_curves) > 0:\n",
        "        # Plot the response curve\n",
        "        fig_response.add_trace(\n",
        "            go.Scatter(\n",
        "                x=ch_curves['SPEND'],\n",
        "                y=ch_curves['PREDICTED_REVENUE'],\n",
        "                mode='lines',\n",
        "                line=dict(color=COLORS['primary'], width=2),\n",
        "                name=ch,\n",
        "                showlegend=False,\n",
        "                hovertemplate='Spend: $%{x:,.0f}/wk<br>Revenue: $%{y:,.0f}/wk<extra></extra>'\n",
        "            ),\n",
        "            row=row, col=col\n",
        "        )\n",
        "        \n",
        "        # Current WEEKLY spend (mean, not sum) - matches how curves are generated\n",
        "        weekly_spend = X_media[ch].mean()\n",
        "        \n",
        "        # Add vertical line at current weekly spend\n",
        "        fig_response.add_vline(\n",
        "            x=weekly_spend,\n",
        "            line=dict(color=COLORS['accent'], width=2, dash='dash'),\n",
        "            row=row, col=col\n",
        "        )\n",
        "\n",
        "# Dynamic height based on rows\n",
        "chart_height = 300 * n_rows\n",
        "\n",
        "fig_response.update_layout(\n",
        "    title=dict(\n",
        "        text=\"<b>Spend → Revenue Response Curves</b><br><sup>Curves flatten = diminishing returns | Orange line = current spend level</sup>\",\n",
        "        x=0.5, xanchor='center'\n",
        "    ),\n",
        "    height=chart_height,\n",
        "    showlegend=False,\n",
        "    plot_bgcolor='rgba(0,0,0,0)',\n",
        "    paper_bgcolor='rgba(0,0,0,0)'\n",
        ")\n",
        "\n",
        "# Update axes for all subplots\n",
        "for i in range(1, n_channels + 1):\n",
        "    row = (i - 1) // n_cols + 1\n",
        "    col = (i - 1) % n_cols + 1\n",
        "    fig_response.update_xaxes(tickformat='$,.0s', gridcolor='lightgray', title_text=\"$ Spend / Week →\", row=row, col=col)\n",
        "    fig_response.update_yaxes(tickformat='$,.0s', gridcolor='lightgray', title_text=\"← $ Revenue / Week\", row=row, col=col)\n",
        "\n",
        "st.plotly_chart(fig_response, use_container_width=True)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# TABLE: Channel Response Curve Metrics (Technical Analysis)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "st.subheader(\"6. Channel Response Curve Analysis\")\n",
        "st.markdown(\"\"\"\n",
        "**Technical metrics from the fitted response curves.** These help identify where each channel sits \n",
        "on its diminishing returns curve and the key parameters driving the shape.\n",
        "\"\"\")\n",
        "\n",
        "# Build metrics table from response curves and model parameters\n",
        "channel_metrics = []\n",
        "\n",
        "for ch in selected_channels:  # Use the same channels shown in response curves\n",
        "    p = best_params[ch]\n",
        "    \n",
        "    # Current weekly spend\n",
        "    weekly_spend = X_media[ch].mean()\n",
        "    \n",
        "    # Get response curve data for this channel\n",
        "    ch_curves = response_curves[response_curves['CHANNEL'] == ch].sort_values('SPEND')\n",
        "    \n",
        "    if len(ch_curves) > 0:\n",
        "        # Revenue ceiling (max of predicted revenue)\n",
        "        revenue_ceiling = ch_curves['PREDICTED_REVENUE'].max()\n",
        "        \n",
        "        # Current revenue (interpolate from curve at current spend)\n",
        "        closest_idx = (ch_curves['SPEND'] - weekly_spend).abs().argmin()\n",
        "        current_revenue = ch_curves.iloc[closest_idx]['PREDICTED_REVENUE']\n",
        "        \n",
        "        # Marginal ROI at current spend\n",
        "        current_marginal_roi = ch_curves.iloc[closest_idx]['MARGINAL_ROI_AT_SPEND']\n",
        "        \n",
        "        # % of ceiling captured\n",
        "        pct_of_ceiling = (current_revenue / revenue_ceiling * 100) if revenue_ceiling > 0 else 0\n",
        "        \n",
        "        # Half-saturation point (gamma) - spend level where you hit 50% of max response\n",
        "        # This is where diminishing returns really accelerate\n",
        "        half_sat_spend = p['gamma']\n",
        "        \n",
        "        # How far past half-saturation are we? (negative = room to grow, positive = saturated)\n",
        "        saturation_ratio = weekly_spend / half_sat_spend if half_sat_spend > 0 else 0\n",
        "        \n",
        "        # Classify position\n",
        "        if saturation_ratio < 0.5:\n",
        "            position = \"🟢 Early (high growth)\"\n",
        "        elif saturation_ratio < 1.0:\n",
        "            position = \"🟡 Approaching inflection\"\n",
        "        elif saturation_ratio < 2.0:\n",
        "            position = \"🟠 Past inflection\"\n",
        "        else:\n",
        "            position = \"🔴 Deep saturation\"\n",
        "        \n",
        "        channel_metrics.append({\n",
        "            'Channel': ch.replace('_GLOBAL_ALL', ''),\n",
        "            'Spend/Wk': f\"${weekly_spend:,.0f}\",\n",
        "            'Revenue Ceiling': f\"${revenue_ceiling:,.0f}/wk\",\n",
        "            '% of Ceiling': f\"{pct_of_ceiling:.0f}%\",\n",
        "            'Marginal ROI': f\"{current_marginal_roi:.2f}x\",\n",
        "            'Half-Sat Point (γ)': f\"${half_sat_spend:,.0f}\",\n",
        "            'vs Half-Sat': f\"{saturation_ratio:.1f}x\",\n",
        "            'Decay Rate (θ)': f\"{p['theta']:.2f}\",\n",
        "            'Position': position\n",
        "        })\n",
        "\n",
        "metrics_df = pd.DataFrame(channel_metrics)\n",
        "\n",
        "# Add sorting options\n",
        "sort_col1, sort_col2 = st.columns([2, 2])\n",
        "with sort_col1:\n",
        "    sort_by = st.selectbox(\n",
        "        \"Sort table by\",\n",
        "        options=['Spend/Wk', 'Marginal ROI', 'vs Half-Sat', '% of Ceiling', 'Position'],\n",
        "        index=0\n",
        "    )\n",
        "\n",
        "# Create numeric columns for sorting\n",
        "metrics_df['_spend_num'] = metrics_df['Spend/Wk'].str.replace(r'[$,]', '', regex=True).astype(float)\n",
        "metrics_df['_marginal_num'] = metrics_df['Marginal ROI'].str.replace('x', '').astype(float)\n",
        "metrics_df['_vs_halfsat_num'] = metrics_df['vs Half-Sat'].str.replace('x', '').astype(float)\n",
        "metrics_df['_pct_ceiling_num'] = metrics_df['% of Ceiling'].str.replace('%', '').astype(float)\n",
        "\n",
        "sort_mapping = {\n",
        "    'Spend/Wk': ('_spend_num', False),\n",
        "    'Marginal ROI': ('_marginal_num', False),\n",
        "    'vs Half-Sat': ('_vs_halfsat_num', True),\n",
        "    '% of Ceiling': ('_pct_ceiling_num', False),\n",
        "    'Position': ('_vs_halfsat_num', True)\n",
        "}\n",
        "sort_field, ascending = sort_mapping[sort_by]\n",
        "metrics_df = metrics_df.sort_values(sort_field, ascending=ascending)\n",
        "\n",
        "# Drop helper columns\n",
        "display_df = metrics_df.drop(columns=['_spend_num', '_marginal_num', '_vs_halfsat_num', '_pct_ceiling_num'])\n",
        "\n",
        "st.dataframe(\n",
        "    display_df,\n",
        "    use_container_width=True,\n",
        "    hide_index=True\n",
        ")\n",
        "\n",
        "# Explanation of key columns\n",
        "with st.expander(\"📖 How to interpret these metrics\"):\n",
        "    st.markdown(\"\"\"\n",
        "    | Metric | What it means |\n",
        "    |--------|---------------|\n",
        "    | **Spend/Wk** | Your current average weekly spend on this channel |\n",
        "    | **Revenue Ceiling** | Maximum weekly revenue this channel can deliver (curve asymptote) |\n",
        "    | **% of Ceiling** | How much of the channel's potential you're currently capturing |\n",
        "    | **Marginal ROI** | Return on the *next* dollar spent. <1.0 means losing money on incremental spend |\n",
        "    | **Half-Sat Point (γ)** | The gamma parameter - spend level where you hit 50% of max response. This is where diminishing returns accelerate significantly |\n",
        "    | **vs Half-Sat** | Your spend ÷ half-saturation point. <1.0 = room to grow, >1.0 = past the inflection point |\n",
        "    | **Decay Rate (θ)** | The theta parameter - how much of this week's ad effect carries into next week. Higher = longer-lasting effect (good for awareness channels) |\n",
        "    | **Position** | Quick assessment based on where you sit relative to the half-saturation point |\n",
        "    \n",
        "    **Key insight**: Channels where you're spending BELOW the half-saturation point (vs Half-Sat < 1.0) \n",
        "    have the most headroom for efficient growth. Channels well ABOVE it are in diminishing returns territory.\n",
        "    \"\"\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# CHART: Channel Contribution Treemap\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "st.subheader(\"7. Channel Contribution Breakdown\")\n",
        "\n",
        "treemap_col1, treemap_col2 = st.columns([2, 2])\n",
        "with treemap_col1:\n",
        "    treemap_scope = st.radio(\n",
        "        \"Show channels\",\n",
        "        options=['All Channels', 'Selected Channels Only'],\n",
        "        horizontal=True\n",
        "    )\n",
        "\n",
        "contrib_data = model_results[['CHANNEL_CODE', 'CURRENT_SPEND', 'ROI']].copy()\n",
        "contrib_data['CONTRIBUTION'] = contrib_data['CURRENT_SPEND'] * contrib_data['ROI']\n",
        "contrib_data = contrib_data.groupby('CHANNEL_CODE').agg({\n",
        "    'CURRENT_SPEND': 'sum',\n",
        "    'CONTRIBUTION': 'sum',\n",
        "    'ROI': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Filter to selected channels if requested\n",
        "if treemap_scope == 'Selected Channels Only':\n",
        "    # Map selected channels to channel codes (strip _GLOBAL_ALL suffix)\n",
        "    selected_codes = [ch.replace('_GLOBAL_ALL', '') for ch in selected_channels]\n",
        "    contrib_data = contrib_data[contrib_data['CHANNEL_CODE'].isin(selected_codes)]\n",
        "\n",
        "contrib_data = contrib_data.sort_values('CONTRIBUTION', ascending=False)\n",
        "\n",
        "fig_treemap = px.treemap(\n",
        "    contrib_data,\n",
        "    path=['CHANNEL_CODE'],\n",
        "    values='CONTRIBUTION',\n",
        "    color='ROI',\n",
        "    color_continuous_scale='RdYlGn',\n",
        "    color_continuous_midpoint=1.0,\n",
        "    hover_data={'CURRENT_SPEND': ':$,.0f', 'ROI': ':.2f'}\n",
        ")\n",
        "\n",
        "fig_treemap.update_layout(\n",
        "    title=dict(\n",
        "        text=\"<b>Revenue Contribution by Channel</b><br><sup>Size = attributed revenue | Color = ROI (green = high, red = low)</sup>\",\n",
        "        x=0.5, xanchor='center'\n",
        "    ),\n",
        "    height=500,\n",
        "    coloraxis_colorbar=dict(title=\"ROI\")\n",
        ")\n",
        "st.plotly_chart(fig_treemap, use_container_width=True)\n",
        "\n",
        "st.success(\"All visualizations complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "save_to_snowflake_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 13: Save Results to Snowflake\n",
        "# =============================================================================\n",
        "#\n",
        "# THREE OUTPUT TABLES (all overwrite mode for idempotent execution):\n",
        "#\n",
        "# 1. MMM.MODEL_RESULTS (overwrite mode)\n",
        "#    - Channel-level results with ROI, confidence intervals, and parameters\n",
        "#    - Used by: Streamlit app, analysis views\n",
        "#\n",
        "# 2. MMM.RESPONSE_CURVES (overwrite mode)\n",
        "#    - Detailed spend → revenue curves for visualization\n",
        "#    - 100 points per channel (0 to 3x max spend)\n",
        "#    - Used by: Streamlit \"What-If Simulator\" charts\n",
        "#\n",
        "# 3. MMM.MODEL_METADATA (overwrite mode)\n",
        "#    - Model configuration and quality metrics\n",
        "#    - Tracks: R², MAPE, hyperparameter settings\n",
        "#    - Used for: Model quality monitoring\n",
        "#\n",
        "# All tables use OVERWRITE to ensure clean, idempotent results on each run.\n",
        "# Historical tracking can be done via a separate versioning/archival process.\n",
        "# =============================================================================\n",
        "\n",
        "def save_to_snowflake(session, model_results, response_curves, config, metrics):\n",
        "    \"\"\"\n",
        "    Save model results and response curves to Snowflake.\n",
        "    \n",
        "    OUTPUT TABLES:\n",
        "    - MMM.MODEL_RESULTS: Channel-level ROI with confidence intervals and parameters\n",
        "    - MMM.RESPONSE_CURVES: Detailed curves with CI bands and efficiency zones\n",
        "    - MMM.MODEL_METADATA: Model configuration and quality metrics\n",
        "    \"\"\"\n",
        "    print(\"\\nSaving results to Snowflake...\")\n",
        "    \n",
        "    # 1. Save main results to MMM.MODEL_RESULTS (enhanced schema)\n",
        "    results_clean = model_results.copy()\n",
        "    \n",
        "    # Map DataFrame columns to table columns\n",
        "    results_for_db = pd.DataFrame({\n",
        "        'MODEL_VERSION': results_clean['MODEL_VERSION'],\n",
        "        'CHANNEL': results_clean['CHANNEL_KEY'],\n",
        "        'COEFF_WEIGHT': results_clean['COEFFICIENT_WEIGHT'],\n",
        "        'ROI': results_clean['ROI'],\n",
        "        'MARGINAL_ROI': results_clean['MARGINAL_ROI'],\n",
        "        'OPTIMAL_SPEND': results_clean['OPTIMAL_SPEND_SUGGESTION'],\n",
        "        # Confidence intervals\n",
        "        'ROI_CI_LOWER': results_clean['ROI_CI_LOWER'],\n",
        "        'ROI_CI_UPPER': results_clean['ROI_CI_UPPER'],\n",
        "        'IS_SIGNIFICANT': results_clean['IS_SIGNIFICANT'],\n",
        "        # Learned parameters\n",
        "        'ADSTOCK_DECAY': results_clean['ADSTOCK_DECAY_RATE'],\n",
        "        'SATURATION_ALPHA': results_clean['SATURATION_ALPHA'],\n",
        "        'SATURATION_GAMMA': results_clean['SATURATION_POINT'],\n",
        "        # Model quality\n",
        "        'CV_MAPE': results_clean['MODEL_MAPE_CV'],\n",
        "        'R_SQUARED': results_clean['MODEL_R2_INSAMPLE'],\n",
        "        'N_OBSERVATIONS': results_clean['N_OBSERVATIONS'],\n",
        "        # Spend context\n",
        "        'CURRENT_SPEND': results_clean['CURRENT_SPEND'],\n",
        "        'SPEND_SHARE': results_clean['SPEND_SHARE']\n",
        "    })\n",
        "    \n",
        "    results_sf = session.create_dataframe(results_for_db)\n",
        "    results_sf.write.mode(\"overwrite\").save_as_table(\"MMM.MODEL_RESULTS\")\n",
        "    print(f\"  ✓ Saved {len(results_for_db)} rows to MMM.MODEL_RESULTS\")\n",
        "    \n",
        "    # Legacy ATOMIC table save removed - schema incompatible with flat output\n",
        "    # Use MMM.MODEL_RESULTS as the primary results table\n",
        "    print(f\"  ✓ Skipping legacy ATOMIC.MMM_MODEL_RESULT (use MMM.MODEL_RESULTS instead)\")\n",
        "    \n",
        "    # 2. Save enhanced response curves (with CI bands and efficiency zones)\n",
        "    curves_clean = response_curves.copy()\n",
        "    curves_clean['MODEL_VERSION'] = config.model_version\n",
        "    \n",
        "    curves_sf = session.create_dataframe(curves_clean)\n",
        "    curves_sf.write.mode(\"overwrite\").save_as_table(\"MMM.RESPONSE_CURVES\")\n",
        "    print(f\"  ✓ Saved {len(curves_clean)} rows to MMM.RESPONSE_CURVES\")\n",
        "    print(f\"    → Includes: CI bands, marginal ROI at each point, efficiency zones\")\n",
        "    \n",
        "    # 3. Save model metadata\n",
        "    metadata = pd.DataFrame([{\n",
        "        'MODEL_VERSION': config.model_version,\n",
        "        'MODEL_RUN_DATE': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'GEO_LEVEL': config.geo_level,\n",
        "        'PRODUCT_LEVEL': config.product_level,\n",
        "        'N_CHANNELS': len(model_results),\n",
        "        'R2_INSAMPLE': metrics['in_sample']['R2'],\n",
        "        'MAPE_CV': metrics['cv_mean'].get('MAPE', None),\n",
        "        'NEVERGRAD_BUDGET': config.nevergrad_budget,\n",
        "        'N_BOOTSTRAP': config.n_bootstrap,\n",
        "        'CONFIDENCE_LEVEL': config.confidence_level\n",
        "    }])\n",
        "    \n",
        "    metadata_sf = session.create_dataframe(metadata)\n",
        "    metadata_sf.write.mode(\"overwrite\").save_as_table(\"MMM.MODEL_METADATA\")\n",
        "    print(f\"  ✓ Saved model metadata to MMM.MODEL_METADATA\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SAVE COMPLETE\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nEnhanced outputs include:\")\n",
        "    print(f\"  • 90% confidence intervals on all ROI estimates\")\n",
        "    print(f\"  • Learned adstock decay and saturation parameters per channel\")\n",
        "    print(f\"  • Response curve CI bands and efficiency zone classifications\")\n",
        "\n",
        "# Save to Snowflake\n",
        "save_to_snowflake(session, model_results, response_curves, config, metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "save_transformed_features_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 14a: Save Transformed Features for SQL Inference\n",
        "# =============================================================================\n",
        "#\n",
        "# PURPOSE: Persist the transformed features (adstock + saturation applied) \n",
        "# to enable SQL-based inference using MMM_CHANNEL_ROI model.\n",
        "#\n",
        "# WHY THIS IS NEEDED:\n",
        "# The base model (MMM_CHANNEL_ROI) expects pre-transformed features. SQL cannot\n",
        "# compute adstock transformations row-by-row because adstock requires temporal\n",
        "# context (each value depends on all previous values).\n",
        "#\n",
        "# By saving the transformed features, we enable:\n",
        "#   1. SQL inference: SELECT MMM_CHANNEL_ROI!PREDICT(...) FROM MMM_FEATURES_TRANSFORMED\n",
        "#   2. Historical tracking of features used in training\n",
        "#   3. Debugging/validation of transformation logic\n",
        "#\n",
        "# OUTPUT TABLE: MMM.MMM_FEATURES_TRANSFORMED\n",
        "# =============================================================================\n",
        "\n",
        "def save_transformed_features(session, X_transformed, X_control, y, df, channels, config):\n",
        "    \"\"\"\n",
        "    Save the transformed features (adstock + saturation applied) to Snowflake.\n",
        "    This enables SQL inference using the base MMM_CHANNEL_ROI model.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SAVING TRANSFORMED FEATURES FOR SQL INFERENCE\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Get week dates\n",
        "    weeks = df.groupby('WEEK_START').first().reset_index()['WEEK_START'].values[:len(y)]\n",
        "    \n",
        "    # Sanitize column names for SQL compatibility\n",
        "    import re\n",
        "    def sanitize_sql_identifier(name):\n",
        "        sanitized = re.sub(r'[^A-Za-z0-9_]', '_', name)\n",
        "        if sanitized and sanitized[0].isdigit():\n",
        "            sanitized = '_' + sanitized\n",
        "        return re.sub(r'_+', '_', sanitized)\n",
        "    \n",
        "    # Build the features dataframe\n",
        "    features_df = pd.DataFrame({\n",
        "        'MODEL_VERSION': config.model_version,\n",
        "        'WEEK_START': weeks,\n",
        "        'ACTUAL_REVENUE': y.values\n",
        "    })\n",
        "    \n",
        "    # Add transformed media columns (these have adstock + saturation applied)\n",
        "    for ch in channels:\n",
        "        col_name = sanitize_sql_identifier(ch)\n",
        "        features_df[col_name] = X_transformed[ch].values\n",
        "    \n",
        "    # Add control columns\n",
        "    for col in X_control.columns:\n",
        "        col_name = sanitize_sql_identifier(col)\n",
        "        features_df[col_name] = X_control[col].values\n",
        "    \n",
        "    # Save to Snowflake\n",
        "    features_sf = session.create_dataframe(features_df)\n",
        "    features_sf.write.mode(\"overwrite\").save_as_table(\"MMM.MMM_FEATURES_TRANSFORMED\")\n",
        "    \n",
        "    print(f\"  ✓ Saved {len(features_df)} rows to MMM.MMM_FEATURES_TRANSFORMED\")\n",
        "    print(f\"  Columns: {len(features_df.columns)}\")\n",
        "    print(f\"    - Model version + metadata: 3\")\n",
        "    print(f\"    - Transformed media channels: {len(channels)}\")\n",
        "    print(f\"    - Control variables: {len(X_control.columns)}\")\n",
        "    print(f\"\\n  SQL INFERENCE ENABLED:\")\n",
        "    print(f\"    SELECT MMM.MMM_CHANNEL_ROI!PREDICT(...)\")\n",
        "    print(f\"    FROM MMM.MMM_FEATURES_TRANSFORMED\")\n",
        "    print(f\"    WHERE PMI_INDEX IS NOT NULL;\")\n",
        "    \n",
        "    return features_df\n",
        "\n",
        "# Save transformed features\n",
        "features_saved = save_transformed_features(\n",
        "    session, X_transformed, X_control, y, df, channels, config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "save_to_feature_store_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 14a-ALT: Save Features to Snowflake Feature Store (Demo)\n",
        "# =============================================================================\n",
        "#\n",
        "# Alternative approach using Feature Store for:\n",
        "#   - Built-in versioning and lineage\n",
        "#   - Point-in-time feature retrieval\n",
        "#   - Integration with Model Registry\n",
        "#\n",
        "# =============================================================================\n",
        "\n",
        "from snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\n",
        "\n",
        "def save_to_feature_store(session, X_transformed, y, df, config, scaler):\n",
        "    print('\\n' + '='*60)\n",
        "    print('SAVING TO SNOWFLAKE FEATURE STORE')\n",
        "    print('='*60)\n",
        "    \n",
        "    # Initialize Feature Store (create schema if needed)\n",
        "    fs = FeatureStore(\n",
        "        session=session,\n",
        "        database='GLOBAL_B2B_MMM',\n",
        "        name='MMM_FEATURE_STORE',\n",
        "        default_warehouse='GLOBAL_B2B_MMM_WH',\n",
        "        creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
        "    )\n",
        "    print('  Feature Store initialized')\n",
        "    \n",
        "    # Create Entity (the join key for features)\n",
        "    week_entity = Entity(name='WEEK', join_keys=['WEEK_START'])\n",
        "    fs.register_entity(week_entity)\n",
        "    print('  Entity WEEK registered')\n",
        "    \n",
        "    # Prepare feature data\n",
        "    weeks = df.groupby('WEEK_START').first().reset_index()['WEEK_START'].values[:len(y)]\n",
        "    X_scaled = scaler.transform(X_transformed.values)\n",
        "    \n",
        "    import re\n",
        "    def sanitize(name):\n",
        "        s = re.sub(r'[^A-Za-z0-9_]', '_', name)\n",
        "        return re.sub(r'_+', '_', s).upper()  # UPPERCASE for Snowflake\n",
        "    \n",
        "    features_df = pd.DataFrame({'WEEK_START': weeks})\n",
        "    for idx, col in enumerate(X_transformed.columns):\n",
        "        features_df[sanitize(col)] = X_scaled[:, idx]\n",
        "    \n",
        "    # Create source table for Feature View\n",
        "    version_safe = config.model_version.replace('.', '_')\n",
        "    source_table = f'MMM.MMM_FEATURES_SOURCE_{version_safe}'\n",
        "    session.create_dataframe(features_df).write.mode('overwrite').save_as_table(source_table)\n",
        "    print(f'  Source table created: {source_table}')\n",
        "    \n",
        "    # Create Feature View\n",
        "    fv = FeatureView(\n",
        "        name='MMM_SCALED_FEATURES',\n",
        "        entities=[week_entity],\n",
        "        feature_df=session.table(source_table),\n",
        "        desc=f'Scaled MMM features for model {config.model_version}'\n",
        "    )\n",
        "    \n",
        "    # Register with versioning\n",
        "    registered_fv = fs.register_feature_view(\n",
        "        feature_view=fv,\n",
        "        version=version_safe,\n",
        "        block=True\n",
        "    )\n",
        "    print(f'  Feature View registered: {registered_fv.name} v{registered_fv.version}')\n",
        "    \n",
        "    # Demo: retrieve features\n",
        "    print('\\n  DEMO RETRIEVAL:')\n",
        "    spine_df = session.create_dataframe(pd.DataFrame({'WEEK_START': weeks[:5]}))\n",
        "    retrieved = fs.retrieve_feature_values(\n",
        "        spine_df=spine_df,\n",
        "        features=[registered_fv]\n",
        "    )\n",
        "    print(f'  Retrieved {retrieved.count()} rows from Feature Store')\n",
        "    \n",
        "    return fs, registered_fv\n",
        "\n",
        "# Run Feature Store demo\n",
        "fs, fv = save_to_feature_store(session, X_transformed, y, df, config, scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "register_model_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 14b: Register Model to Snowflake Model Registry\n",
        "# =============================================================================\n",
        "#\n",
        "# SNOWFLAKE MODEL REGISTRY\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# The Model Registry provides centralized model management:\n",
        "#\n",
        "# 1. VERSION TRACKING: Each training run creates a new version\n",
        "# 2. METADATA STORAGE: Hyperparameters, metrics, and lineage\n",
        "# 3. MODEL COMPARISON: Compare performance across versions in Snowsight\n",
        "# 4. DEPLOYMENT: Models can be deployed for inference (optional)\n",
        "#\n",
        "# WHAT WE REGISTER:\n",
        "# - The trained sklearn Ridge model\n",
        "# - Quality metrics (R², CV MAPE)\n",
        "# - Training configuration\n",
        "#\n",
        "# NOTE: sklearn models require `sample_input_data` to infer the model signature.\n",
        "# We pass a small sample of the scaled feature matrix used during training.\n",
        "#\n",
        "# VIEW IN SNOWSIGHT: AI & ML → Models → mmm_channel_roi\n",
        "# =============================================================================\n",
        "\n",
        "def register_model_to_registry(session, model, scaler, X_transformed, config, metrics):\n",
        "    \"\"\"\n",
        "    Register the trained MMM model to Snowflake Model Registry.\n",
        "    \n",
        "    This enables version tracking and model comparison in Snowsight UI.\n",
        "    FAILS HARD if registration doesn't work - this is a critical step.\n",
        "    \n",
        "    Args:\n",
        "        session: Snowflake session\n",
        "        model: Trained sklearn Ridge model\n",
        "        scaler: StandardScaler used to normalize features\n",
        "        X_transformed: DataFrame with transformed features (before scaling)\n",
        "        config: MMMConfig object\n",
        "        metrics: Dictionary with model metrics\n",
        "    \n",
        "    Raises:\n",
        "        Exception: If model registration fails for any reason\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SNOWFLAKE MODEL REGISTRY\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    from snowflake.ml.registry import Registry\n",
        "    print(\"  ✓ Registry imported successfully\")\n",
        "    \n",
        "    # Get current database for explicit registry location\n",
        "    db_name = session.get_current_database()\n",
        "    schema_name = \"MMM\"\n",
        "    print(f\"  Target location: {db_name}.{schema_name}\")\n",
        "    \n",
        "    # Initialize registry with explicit database/schema\n",
        "    print(\"  Initializing Registry...\")\n",
        "    reg = Registry(session=session, database_name=db_name, schema_name=schema_name)\n",
        "    print(\"  ✓ Registry initialized\")\n",
        "    \n",
        "    # Prepare metrics for registry\n",
        "    registry_metrics = {\n",
        "        \"r2_insample\": float(metrics['in_sample']['R2']),\n",
        "        \"mape_cv\": float(metrics['cv_mean'].get('MAPE', 0)),\n",
        "        \"rmse_insample\": float(metrics['in_sample'].get('RMSE', 0)),\n",
        "        \"nrmse_cv\": float(metrics['cv_mean'].get('NRMSE', 0)),\n",
        "    }\n",
        "    \n",
        "    # Clean version name (replace dots and underscores that might cause issues)\n",
        "    version_name = config.model_version.replace(\".\", \"_\").replace(\" \", \"_\")\n",
        "    print(f\"  Version name: {version_name}\")\n",
        "    print(f\"  Model type: {type(model).__name__}\")\n",
        "    \n",
        "    # Create sample input data for model signature inference\n",
        "    # sklearn models require sample_input_data to register with Snowflake ML\n",
        "    # Use first 10 rows of SCALED features (what the model actually sees)\n",
        "    # IMPORTANT: Sanitize column names to be valid SQL identifiers\n",
        "    # Valid SQL identifiers: start with letter/underscore, contain only letters/digits/underscores\n",
        "    import re\n",
        "    def sanitize_sql_identifier(name):\n",
        "        # Replace any non-alphanumeric character (except underscore) with underscore\n",
        "        sanitized = re.sub(r'[^A-Za-z0-9_]', '_', name)\n",
        "        # Ensure it starts with a letter or underscore (not a digit)\n",
        "        if sanitized and sanitized[0].isdigit():\n",
        "            sanitized = '_' + sanitized\n",
        "        # Collapse multiple underscores\n",
        "        sanitized = re.sub(r'_+', '_', sanitized)\n",
        "        return sanitized\n",
        "    \n",
        "    sanitized_columns = [sanitize_sql_identifier(col) for col in X_transformed.columns]\n",
        "    \n",
        "    X_scaled_sample = pd.DataFrame(\n",
        "        scaler.transform(X_transformed.head(10)),\n",
        "        columns=sanitized_columns\n",
        "    )\n",
        "    print(f\"  Sample input shape: {X_scaled_sample.shape}\")\n",
        "    print(f\"  Sample columns (first 5): {sanitized_columns[:5]}\")\n",
        "    \n",
        "    # Log the model to registry\n",
        "    print(\"  Calling reg.log_model()...\")\n",
        "    model_ref = reg.log_model(\n",
        "        model=model,\n",
        "        model_name=\"MMM_CHANNEL_ROI\",\n",
        "        version_name=version_name,\n",
        "        conda_dependencies=[\"scikit-learn\", \"pandas\", \"numpy\"],\n",
        "        target_platforms=[\"WAREHOUSE\"],\n",
        "        metrics=registry_metrics,\n",
        "        sample_input_data=X_scaled_sample,\n",
        "        comment=f\"MMM model trained on {config.input_view} with geo_level={config.geo_level}\"\n",
        "    )\n",
        "    print(f\"  ✓ log_model returned: {model_ref}\")\n",
        "    \n",
        "    print(f\"\\n✓ Model registered to Snowflake Model Registry\")\n",
        "    print(f\"  Database: {db_name}\")\n",
        "    print(f\"  Schema: {schema_name}\")\n",
        "    print(f\"  Model Name: MMM_CHANNEL_ROI\")\n",
        "    print(f\"  Version: {version_name}\")\n",
        "    print(f\"  Metrics: R²={registry_metrics['r2_insample']:.4f}, CV MAPE={registry_metrics['mape_cv']:.1f}%\")\n",
        "    print(f\"\\n  View in Snowsight: AI & ML → Models → {db_name}.{schema_name}.MMM_CHANNEL_ROI\")\n",
        "    \n",
        "    return model_ref\n",
        "\n",
        "# Register model to Snowflake Model Registry\n",
        "# Pass scaler and X_transformed so we can provide sample_input_data for signature inference\n",
        "model_ref = register_model_to_registry(session, model, scaler, X_transformed, config, metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "ml_observability_setup_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 14d: ML Observability - Model Monitoring Setup\n",
        "# =============================================================================\n",
        "#\n",
        "# SNOWFLAKE ML OBSERVABILITY\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# ML Observability tracks model behavior over time:\n",
        "#\n",
        "# 1. DRIFT DETECTION: Monitors if input features shift from training baseline\n",
        "# 2. PERFORMANCE TRACKING: Tracks prediction accuracy as ground truth arrives\n",
        "# 3. SEGMENT ANALYSIS: Monitor performance by region, channel, etc.\n",
        "#\n",
        "# WHAT WE SET UP:\n",
        "# - Predictions logging table (MMM_MODEL_PREDICTIONS)\n",
        "# - Model Monitor attached to the registered model\n",
        "# - Segment monitoring by SUPER_REGION\n",
        "#\n",
        "# VIEW IN SNOWSIGHT: AI & ML → Models → MMM_CHANNEL_ROI → Monitors\n",
        "# \n",
        "# Docs: https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/model-observability\n",
        "# =============================================================================\n",
        "\n",
        "def setup_ml_observability(session, config, X_transformed, y, y_pred, df_input):\n",
        "    \"\"\"\n",
        "    Set up ML Observability for the MMM model.\n",
        "    \n",
        "    Creates:\n",
        "    1. Predictions logging table with features, predictions, and actuals\n",
        "    2. Baseline table for drift comparison\n",
        "    3. Model Monitor for ongoing tracking\n",
        "    \n",
        "    Args:\n",
        "        session: Snowflake session\n",
        "        config: MMMConfig object\n",
        "        X_transformed: Transformed features DataFrame\n",
        "        y: Actual revenue values\n",
        "        y_pred: Model predictions\n",
        "        df_input: Original input DataFrame (for segment columns)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ML OBSERVABILITY SETUP\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    db_name = session.get_current_database()\n",
        "    schema_name = \"MMM\"\n",
        "    model_name = \"MMM_CHANNEL_ROI\"\n",
        "    version_name = config.model_version.replace(\".\", \"_\").replace(\" \", \"_\")\n",
        "    monitor_name = f\"MMM_MONITOR_{version_name}\".upper()\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 1: Create Predictions Logging Table\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n[1/4] Creating predictions logging table...\")\n",
        "    \n",
        "    # Prepare prediction data with required columns\n",
        "    pred_df = pd.DataFrame({\n",
        "        'PREDICTION_ID': [f\"pred_{i}_{datetime.now().strftime('%Y%m%d%H%M%S')}\" for i in range(len(y))],\n",
        "        'PREDICTION_TS': pd.Timestamp.now(),\n",
        "        'PREDICTION': y_pred.flatten() if hasattr(y_pred, 'flatten') else y_pred,\n",
        "        'ACTUAL': y.values if hasattr(y, 'values') else y,\n",
        "    })\n",
        "    \n",
        "    # Add segment column (SUPER_REGION) if available\n",
        "    if 'SUPER_REGION' in df_input.columns:\n",
        "        pred_df['SUPER_REGION'] = df_input['SUPER_REGION'].values[:len(pred_df)]\n",
        "        segment_col = 'SUPER_REGION'\n",
        "    else:\n",
        "        segment_col = None\n",
        "    \n",
        "    # Add key features for drift monitoring (top 5 media channels by spend)\n",
        "    feature_cols = []\n",
        "    for i, col in enumerate(X_transformed.columns[:10]):  # Top 10 features\n",
        "        safe_col = f\"FEATURE_{i}\"\n",
        "        pred_df[safe_col] = X_transformed[col].values[:len(pred_df)]\n",
        "        feature_cols.append(safe_col)\n",
        "    \n",
        "    print(f\"  Prediction records: {len(pred_df)}\")\n",
        "    print(f\"  Feature columns: {len(feature_cols)}\")\n",
        "    print(f\"  Segment column: {segment_col or 'None'}\")\n",
        "    \n",
        "    # Convert to Snowpark DataFrame and write\n",
        "    sp_pred_df = session.create_dataframe(pred_df)\n",
        "    \n",
        "    predictions_table = f\"{db_name}.{schema_name}.MMM_MODEL_PREDICTIONS\"\n",
        "    sp_pred_df.write.mode(\"append\").save_as_table(predictions_table)\n",
        "    print(f\"  ✓ Appended to: {predictions_table}\")\n",
        "    \n",
        "    # Ensure PREDICTION_TS is TIMESTAMP_NTZ (required by Model Monitor)\n",
        "    session.sql(f\"\"\"\n",
        "        ALTER TABLE {predictions_table} \n",
        "        ALTER COLUMN PREDICTION_TS SET DATA TYPE TIMESTAMP_NTZ\n",
        "    \"\"\").collect()\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 2: Create Baseline Table (for drift comparison)\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n[2/4] Creating baseline table for drift detection...\")\n",
        "    \n",
        "    baseline_table = f\"{db_name}.{schema_name}.MMM_MODEL_BASELINE\"\n",
        "    \n",
        "    # Use first 70% of data as baseline (training period)\n",
        "    baseline_size = int(len(pred_df) * 0.7)\n",
        "    baseline_df = pred_df.head(baseline_size).copy()\n",
        "    \n",
        "    # Baseline only needs to be created once - don't overwrite if exists\n",
        "    baseline_exists = len(session.sql(f\"SHOW TABLES LIKE 'MMM_MODEL_BASELINE' IN SCHEMA {db_name}.{schema_name}\").collect()) > 0\n",
        "    \n",
        "    if not baseline_exists:\n",
        "        sp_baseline_df = session.create_dataframe(baseline_df)\n",
        "        sp_baseline_df.write.mode(\"overwrite\").save_as_table(baseline_table)\n",
        "        print(f\"  ✓ Created: {baseline_table} ({baseline_size} rows)\")\n",
        "        \n",
        "        session.sql(f\"\"\"\n",
        "            ALTER TABLE {baseline_table} \n",
        "            ALTER COLUMN PREDICTION_TS SET DATA TYPE TIMESTAMP_NTZ\n",
        "        \"\"\").collect()\n",
        "    else:\n",
        "        print(f\"  ✓ Baseline table already exists: {baseline_table} (preserved)\")\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 3: Create Model Monitor (one per model, not per version)\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n[3/4] Creating Model Monitor...\")\n",
        "    \n",
        "    # Use a single monitor name (not version-specific) so we don't create many monitors\n",
        "    monitor_name = \"MMM_MONITOR\"\n",
        "    \n",
        "    # Check if monitor already exists\n",
        "    existing_monitors = session.sql(f\"SHOW MODEL MONITORS LIKE '{monitor_name}' IN SCHEMA {db_name}.{schema_name}\").collect()\n",
        "    \n",
        "    if len(existing_monitors) > 0:\n",
        "        print(f\"  ✓ Monitor already exists: {monitor_name} (preserved)\")\n",
        "    else:\n",
        "        # Create the Model Monitor\n",
        "        # Syntax: https://docs.snowflake.com/en/sql-reference/sql/create-model-monitor\n",
        "        segment_clause = f\"SEGMENT_COLUMNS = ('{segment_col}')\" if segment_col else \"\"\n",
        "        \n",
        "        create_monitor_sql = f\"\"\"\n",
        "        CREATE MODEL MONITOR {schema_name}.{monitor_name} WITH\n",
        "            MODEL = {model_name}\n",
        "            VERSION = '{version_name}'\n",
        "            FUNCTION = 'predict'\n",
        "            SOURCE = {predictions_table}\n",
        "            WAREHOUSE = {session.get_current_warehouse()}\n",
        "            REFRESH_INTERVAL = '1 hour'\n",
        "            AGGREGATION_WINDOW = '1 day'\n",
        "            TIMESTAMP_COLUMN = PREDICTION_TS\n",
        "            ID_COLUMNS = ('PREDICTION_ID')\n",
        "            BASELINE = {baseline_table}\n",
        "            PREDICTION_SCORE_COLUMNS = ('PREDICTION')\n",
        "            ACTUAL_SCORE_COLUMNS = ('ACTUAL')\n",
        "            {segment_clause}\n",
        "        \"\"\"\n",
        "        \n",
        "        try:\n",
        "            session.sql(create_monitor_sql).collect()\n",
        "            print(f\"  ✓ Created Model Monitor: {monitor_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠ Monitor creation failed: {str(e)}\")\n",
        "            print(\"    (This may require additional privileges or feature enablement)\")\n",
        "            return None\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 4: Verify and Display Info\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n[4/4] Verifying Model Monitor...\")\n",
        "    \n",
        "    try:\n",
        "        monitor_info = session.sql(f\"DESCRIBE MODEL MONITOR {db_name}.{schema_name}.{monitor_name}\").collect()\n",
        "        print(f\"  ✓ Monitor is active\")\n",
        "        \n",
        "        # Show monitor status\n",
        "        for row in monitor_info:\n",
        "            if hasattr(row, 'property') and hasattr(row, 'value'):\n",
        "                print(f\"    {row['property']}: {row['value']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Monitor verification: {str(e)[:80]}\")\n",
        "    \n",
        "    print(f\"\"\"\n",
        "✓ ML OBSERVABILITY CONFIGURED\n",
        "─────────────────────────────────────────────────────────────────────────────\n",
        "  Model:          {model_name} (version: {version_name})\n",
        "  Monitor:        {monitor_name}\n",
        "  Predictions:    {predictions_table}\n",
        "  Baseline:       {baseline_table}\n",
        "  Segments:       {segment_col or 'None'}\n",
        "  \n",
        "  METRICS TRACKED:\n",
        "  • Performance:  RMSE, MAE, R² (comparing predictions to actuals)\n",
        "  • Drift:        PSI, JS Divergence (feature distribution shifts)\n",
        "  • Volume:       Prediction counts, null rates\n",
        "  \n",
        "  VIEW IN SNOWSIGHT:\n",
        "  AI & ML → Models → {model_name} → Monitors → {monitor_name}\n",
        "  \n",
        "  QUERY METRICS (SQL):\n",
        "  SELECT * FROM TABLE(MODEL_MONITOR_PERFORMANCE_METRIC(\n",
        "      '{monitor_name}', 'RMSE', 'DAY',\n",
        "      DATEADD('day', -30, CURRENT_TIMESTAMP()),\n",
        "      CURRENT_TIMESTAMP()\n",
        "  ));\n",
        "─────────────────────────────────────────────────────────────────────────────\n",
        "\"\"\")\n",
        "    \n",
        "    return monitor_name\n",
        "\n",
        "\n",
        "# Set up ML Observability\n",
        "# Only run if model was successfully registered\n",
        "if 'model_ref' in dir() and model_ref is not None:\n",
        "    # Get predictions for logging\n",
        "    y_pred_final = model.predict(scaler.transform(X_transformed))\n",
        "    \n",
        "    monitor_name = setup_ml_observability(\n",
        "        session=session,\n",
        "        config=config,\n",
        "        X_transformed=X_transformed,\n",
        "        y=y,\n",
        "        y_pred=y_pred_final,\n",
        "        df_input=df\n",
        "    )\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # Set this version as the default (uncomment to enable)\n",
        "    # -------------------------------------------------------------------------\n",
        "    version_name = config.model_version.replace(\".\", \"_\").replace(\" \", \"_\").upper()\n",
        "    session.sql(f\"ALTER MODEL MMM.MMM_CHANNEL_ROI SET DEFAULT_VERSION = '{version_name}'\").collect()\n",
        "    print(f\"✓ Set {version_name} as default model version\")\n",
        "else:\n",
        "    print(\"⚠ Skipping ML Observability setup - model not registered\")\n",
        "    monitor_name = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "executive_summary_display"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 14: Executive Summary\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"                    MMM TRAINING EXECUTIVE SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\"\"\n",
        "MODEL CONFIGURATION\n",
        "-------------------\n",
        "Version:           {config.model_version}\n",
        "Geographic Level:  {config.geo_level}\n",
        "Product Level:     {config.product_level}\n",
        "Channels Modeled:  {len(channels)}\n",
        "\n",
        "MODEL QUALITY\n",
        "-------------\n",
        "In-Sample R²:      {metrics['in_sample']['R2']:.4f}\n",
        "CV MAPE:           {metrics['cv_mean'].get('MAPE', 0):.1f}% ± {metrics['cv_std'].get('MAPE', 0):.1f}%\n",
        "CV R²:             {metrics['cv_mean'].get('R2', 0):.4f} ± {metrics['cv_std'].get('R2', 0):.4f}\n",
        "\n",
        "TOP PERFORMING CHANNELS (by ROI)\n",
        "--------------------------------\"\"\")\n",
        "\n",
        "ci_level = int(config.confidence_level * 100)\n",
        "top_channels = roi_confidence.head(5)\n",
        "for _, row in top_channels.iterrows():\n",
        "    sig = \"*\" if row['IS_SIGNIFICANT'] else \"\"\n",
        "    print(f\"  {row['CHANNEL_KEY']}: ROI = {row['ROI_MEAN']:.2f} [{row[f'ROI_CI_LOWER_{ci_level}']:.2f}, {row[f'ROI_CI_UPPER_{ci_level}']:.2f}] {sig}\")\n",
        "\n",
        "print(f\"\"\"\n",
        "BUDGET OPTIMIZATION HIGHLIGHTS\n",
        "------------------------------\n",
        "Constraint: ±{config.budget_change_limit*100:.0f}% per channel (budget neutral)\"\"\")\n",
        "\n",
        "if len(budget_recommendations) > 0:\n",
        "    top_increase = budget_recommendations.head(3)\n",
        "    top_decrease = budget_recommendations.tail(3)\n",
        "    \n",
        "    print(\"\\nIncrease Spend:\")\n",
        "    for _, row in top_increase.iterrows():\n",
        "        print(f\"  {row['CHANNEL_KEY']}: +{row['CHANGE_PCT']:.1f}% (${row['CHANGE_AMOUNT']:,.0f})\")\n",
        "    \n",
        "    print(\"\\nDecrease Spend:\")\n",
        "    for _, row in top_decrease.iterrows():\n",
        "        print(f\"  {row['CHANNEL_KEY']}: {row['CHANGE_PCT']:.1f}% (${row['CHANGE_AMOUNT']:,.0f})\")\n",
        "\n",
        "monitor_status = f\"Active ({monitor_name})\" if monitor_name else \"Not configured\"\n",
        "\n",
        "print(f\"\"\"\n",
        "OUTPUT TABLES & REGISTRY\n",
        "------------------------\n",
        "• MMM.MODEL_RESULTS          - Channel ROI with confidence intervals\n",
        "• MMM.RESPONSE_CURVES        - Spend vs. revenue curves  \n",
        "• MMM.MODEL_METADATA         - Model configuration and quality\n",
        "• MMM.MMM_MODEL_PREDICTIONS  - Predictions for ML Observability\n",
        "• Model Registry             - MMM_CHANNEL_ROI (Snowsight: AI & ML → Models)\n",
        "• Model Monitor              - {monitor_status}\n",
        "\n",
        "ML OBSERVABILITY\n",
        "----------------\n",
        "• Drift Detection:           Monitors feature distribution shifts\n",
        "• Performance Tracking:      RMSE, MAE, R² over time\n",
        "• Segment Analysis:          By SUPER_REGION\n",
        "\n",
        "NEXT STEPS\n",
        "----------\n",
        "1. Review results in DIMENSIONAL.V_MMM_RESULTS_ANALYSIS view\n",
        "2. Visualize response curves in Streamlit app\n",
        "3. Use Budget Optimizer for scenario planning\n",
        "4. Compare model versions in Model Registry\n",
        "5. Monitor model health in Snowsight (AI & ML → Models → Monitors)\n",
        "6. Run 02_snowflake_ml_features.ipynb for FORECAST baseline comparison\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"Model training completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 15: Fix Ownership (if run as ACCOUNTADMIN)\n",
        "# =============================================================================\n",
        "# If you ran this notebook as ACCOUNTADMIN, the tables won't be accessible\n",
        "# to the project role. This cell fixes ownership so the Streamlit app works.\n",
        "\n",
        "current_role = session.sql(\"SELECT CURRENT_ROLE()\").collect()[0][0]\n",
        "\n",
        "if current_role in (\"ACCOUNTADMIN\", \"SYSADMIN\"):\n",
        "    print(f\"Detected {current_role} - transferring ownership to project role...\")\n",
        "    \n",
        "    project_role = \"GLOBAL_B2B_MMM_ROLE\"\n",
        "    database = \"GLOBAL_B2B_MMM\"\n",
        "    \n",
        "    ownership_grants = [\n",
        "        f\"GRANT OWNERSHIP ON TABLE {database}.MMM.MODEL_RESULTS TO ROLE {project_role} COPY CURRENT GRANTS\",\n",
        "        f\"GRANT OWNERSHIP ON TABLE {database}.MMM.RESPONSE_CURVES TO ROLE {project_role} COPY CURRENT GRANTS\",\n",
        "        f\"GRANT OWNERSHIP ON TABLE {database}.MMM.MODEL_METADATA TO ROLE {project_role} COPY CURRENT GRANTS\",\n",
        "        f\"GRANT OWNERSHIP ON TABLE {database}.MMM.MMM_MODEL_PREDICTIONS TO ROLE {project_role} COPY CURRENT GRANTS\",\n",
        "    ]\n",
        "    \n",
        "    for grant in ownership_grants:\n",
        "        try:\n",
        "            session.sql(grant).collect()\n",
        "            table_name = grant.split(\"ON TABLE \")[1].split(\" TO\")[0]\n",
        "            print(f\"  ✓ Transferred: {table_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠ Skipped (may not exist): {e}\")\n",
        "    \n",
        "    print(f\"\\nOwnership transferred to {project_role}. Streamlit app should now work.\")\n",
        "else:\n",
        "    print(f\"Running as {current_role} - no ownership fix needed.\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
