"""
Cortex Analyst Integration for Structured Data Queries.

Routes quantitative questions about marketing data to Snowflake Cortex Analyst
using the MMM semantic model for accurate SQL generation.
"""
import streamlit as st
import pandas as pd
import json
import logging
from typing import Optional, Tuple

logger = logging.getLogger(__name__)

# Semantic model configuration
SEMANTIC_MODEL_PATH = "@MMM.SEMANTIC_MODELS/mmm_roi_model.yaml"


def send_analyst_message(
    session,
    prompt: str,
    semantic_model_path: str = SEMANTIC_MODEL_PATH,
    message_history: Optional[list] = None
) -> dict:
    """
    Send a message to Cortex Analyst and get a response.
    
    Args:
        session: Snowflake Snowpark session
        prompt: The user's natural language question
        semantic_model_path: Path to the semantic model YAML in a stage
        message_history: Optional conversation history for context
        
    Returns:
        dict with 'content' (list of response items) and optionally 'request_id'
    """
    if message_history is None:
        message_history = []
    
    # Build the messages payload
    messages = message_history + [{"role": "user", "content": [{"type": "text", "text": prompt}]}]
    
    try:
        # Call Cortex Analyst via _SNOWFLAKE.CORTEX.COMPLETE_ANALYST
        # This is the recommended approach for Streamlit in Snowflake
        request_body = {
            "messages": messages,
            "semantic_model_file": semantic_model_path
        }
        
        result = session.sql(f"""
            SELECT SNOWFLAKE.CORTEX.COMPLETE(
                'analyst',
                PARSE_JSON($${json.dumps(request_body)}$$)
            ) as response
        """).collect()
        
        if result and len(result) > 0:
            response_str = result[0]['RESPONSE']
            response = json.loads(response_str) if isinstance(response_str, str) else response_str
            return response
        else:
            return {"content": [{"type": "text", "text": "No response received from Analyst."}]}
            
    except Exception as e:
        logger.error(f"Cortex Analyst error: {e}")
        # Return a graceful error message
        return {
            "content": [
                {
                    "type": "text", 
                    "text": f"Unable to process query. Please ensure Cortex Analyst is configured. Error: {str(e)}"
                }
            ]
        }


def parse_analyst_response(response: dict) -> Tuple[str, Optional[str], Optional[pd.DataFrame]]:
    """
    Parse a Cortex Analyst response into displayable components.
    
    Args:
        response: The raw response dict from Cortex Analyst
        
    Returns:
        Tuple of (text_response, sql_query, result_dataframe)
    """
    text_parts = []
    sql_query = None
    result_df = None
    
    content = response.get("content", [])
    
    for item in content:
        item_type = item.get("type", "")
        
        if item_type == "text":
            text_parts.append(item.get("text", ""))
            
        elif item_type == "sql":
            sql_query = item.get("statement", "")
            
        elif item_type == "suggestions":
            suggestions = item.get("suggestions", [])
            if suggestions:
                text_parts.append("\n**Suggested follow-ups:**")
                for s in suggestions:
                    text_parts.append(f"- {s}")
                    
        elif item_type == "results":
            # Some responses include inline results
            data = item.get("data", [])
            if data:
                result_df = pd.DataFrame(data)
    
    return "\n".join(text_parts), sql_query, result_df


def execute_analyst_sql(session, sql: str) -> pd.DataFrame:
    """
    Execute SQL generated by Cortex Analyst.
    
    Args:
        session: Snowflake Snowpark session
        sql: SQL query string
        
    Returns:
        DataFrame with results
    """
    try:
        return session.sql(sql).to_pandas()
    except Exception as e:
        logger.error(f"SQL execution error: {e}")
        raise


def render_analyst_chat(session, key_prefix: str = "analyst"):
    """
    Render a chat interface for Cortex Analyst.
    
    This provides a reusable chat component that can be embedded in any page.
    
    Args:
        session: Snowflake Snowpark session
        key_prefix: Unique prefix for session state keys
    """
    # Initialize session state for chat history
    history_key = f"{key_prefix}_history"
    if history_key not in st.session_state:
        st.session_state[history_key] = []
    
    # Display chat history
    for msg in st.session_state[history_key]:
        role = msg.get("role", "user")
        content = msg.get("content", [])
        
        with st.chat_message(role):
            for item in content:
                if item.get("type") == "text":
                    st.markdown(item.get("text", ""))
                elif item.get("type") == "sql":
                    st.code(item.get("statement", ""), language="sql")
                elif item.get("type") == "dataframe":
                    st.dataframe(item.get("data"), use_container_width=True)
    
    # Chat input
    if prompt := st.chat_input("Ask about ROI, spend, or channel performance...", key=f"{key_prefix}_input"):
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Add to history
        user_msg = {"role": "user", "content": [{"type": "text", "text": prompt}]}
        st.session_state[history_key].append(user_msg)
        
        # Get response
        with st.chat_message("assistant"):
            with st.spinner("Analyzing..."):
                response = send_analyst_message(
                    session,
                    prompt,
                    message_history=st.session_state[history_key][:-1]  # Exclude current msg
                )
                
                text_resp, sql_query, result_df = parse_analyst_response(response)
                
                # Display text response
                if text_resp:
                    st.markdown(text_resp)
                
                # Display and optionally execute SQL
                if sql_query:
                    st.code(sql_query, language="sql")
                    
                    # Execute the SQL and show results
                    try:
                        df = execute_analyst_sql(session, sql_query)
                        st.dataframe(df, use_container_width=True)
                        
                        # Store dataframe in response for history
                        response["content"].append({
                            "type": "dataframe",
                            "data": df
                        })
                    except Exception as e:
                        st.error(f"Could not execute query: {e}")
        
        # Add assistant response to history
        assistant_msg = {"role": "assistant", "content": response.get("content", [])}
        st.session_state[history_key].append(assistant_msg)
        
        st.rerun()


# =============================================================================
# DIAGNOSTIC NARRATIVE GENERATION (Layer 3: AI Interpretation)
# =============================================================================

def generate_diagnostic_narrative(
    session,
    context_type: str,
    channel: str,
    metrics: dict,
    static_context: str = ""
) -> Optional[str]:
    """
    Generate AI interpretation of diagnostic metrics using Snowflake Cortex COMPLETE.
    
    This function creates Layer 3 of the three-layer diagnostic approach:
    - Layer 1: Static context (what the metric is, why it matters)
    - Layer 2: Quantitative metrics (calculated from data)
    - Layer 3: AI narrative (this function) - interprets metrics in context
    
    Args:
        session: Snowflake Snowpark session
        context_type: Type of diagnostic - "response_curve" or "marginal_efficiency"
        channel: The marketing channel being analyzed
        metrics: Dict with quantitative metrics (current_spend, saturation_pct, marginal_roi, headroom, etc.)
        static_context: Optional static explanation text to include in prompt
        
    Returns:
        AI-generated interpretation string, or None if generation fails
    """
    # Build context-specific prompts
    if context_type == "response_curve":
        prompt = f"""You are a marketing analytics expert advising a B2B enterprise marketing leader. 
Based on the following response curve metrics, provide a 2-3 sentence actionable interpretation.
Focus on what the marketing leader should DO based on these numbers.

CHANNEL: {channel}

RESPONSE CURVE METRICS:
- Current Weekly Spend: ${metrics.get('current_spend', 0):,.0f}
- Saturation Level: {metrics.get('saturation_pct', 0):.0f}% (0%=highly efficient, 100%=fully saturated)
- Marginal ROI at Current Spend: {metrics.get('marginal_roi', 0):.2f}x (revenue per incremental dollar)
- Optimal Spend Range: ${metrics.get('optimal_lower', 0):,.0f} - ${metrics.get('optimal_upper', 0):,.0f}
- Headroom Before Saturation: ${metrics.get('headroom', 0):,.0f}

CONTEXT: Response curves show diminishing returns - each additional dollar generates progressively less revenue.

Provide a specific, actionable interpretation for {channel}:"""

    elif context_type == "marginal_efficiency":
        prompt = f"""You are a marketing analytics expert advising a B2B enterprise marketing leader.
Based on the following marginal efficiency metrics, provide a 2-3 sentence actionable recommendation.
Be specific about whether to increase, maintain, or decrease spend.

CHANNEL: {channel}

MARGINAL EFFICIENCY METRICS:
- Current Marginal ROI: {metrics.get('current_mroi', 0):.2f}x (revenue per next dollar spent)
- Peak Efficiency Spend Level: ${metrics.get('peak_spend', 0):,.0f}
- Breakeven Point: ${metrics.get('breakeven_spend', 0):,.0f} (where marginal ROI = 1.0x)
- Efficiency Ranking: #{metrics.get('rank', 'N/A')} among active channels

CONTEXT: Marginal ROI above 1.0x means each additional dollar returns more than $1 in revenue.
Above 1.5x is considered highly efficient. Below 0.8x indicates saturation.

Provide a specific budget recommendation for {channel}:"""

    else:
        prompt = f"""You are a marketing analytics expert. Interpret these metrics for {channel}:
{json.dumps(metrics, indent=2, default=str)}

Provide a 2-3 sentence actionable interpretation:"""

    try:
        # Use Cortex COMPLETE for text generation
        # Escape single quotes in prompt for SQL
        safe_prompt = prompt.replace("'", "''")
        
        result = session.sql(f"""
            SELECT SNOWFLAKE.CORTEX.COMPLETE(
                'mistral-large',
                '{safe_prompt}'
            ) as response
        """).collect()
        
        if result and len(result) > 0:
            response = result[0]['RESPONSE']
            # Clean up the response (remove any leading/trailing whitespace)
            if response:
                return response.strip()
        
        return None
        
    except Exception as e:
        logger.error(f"Cortex COMPLETE error for diagnostic narrative: {e}")
        return None


def generate_comparative_narrative(
    session,
    channels: list,
    metrics_by_channel: dict
) -> Optional[str]:
    """
    Generate AI interpretation comparing multiple channels.
    
    Args:
        session: Snowflake Snowpark session
        channels: List of channel names to compare
        metrics_by_channel: Dict mapping channel name to its metrics dict
        
    Returns:
        AI-generated comparative analysis, or None if generation fails
    """
    # Build comparison table for prompt
    comparison_lines = []
    for ch in channels:
        m = metrics_by_channel.get(ch, {})
        comparison_lines.append(
            f"- {ch}: mROI={m.get('marginal_roi', 0):.2f}x, "
            f"Saturation={m.get('saturation_pct', 0):.0f}%, "
            f"Headroom=${m.get('headroom', 0):,.0f}"
        )
    
    comparison_text = "\n".join(comparison_lines)
    
    prompt = f"""You are a marketing analytics expert advising on B2B budget allocation.
Compare these channels and recommend how to reallocate budget for maximum ROI:

{comparison_text}

Provide a 2-3 sentence recommendation on which channel(s) to increase and which to decrease:"""

    try:
        safe_prompt = prompt.replace("'", "''")
        
        result = session.sql(f"""
            SELECT SNOWFLAKE.CORTEX.COMPLETE(
                'mistral-large',
                '{safe_prompt}'
            ) as response
        """).collect()
        
        if result and len(result) > 0:
            response = result[0]['RESPONSE']
            if response:
                return response.strip()
        
        return None
        
    except Exception as e:
        logger.error(f"Cortex COMPLETE error for comparative narrative: {e}")
        return None


# Example queries for user guidance
EXAMPLE_QUERIES = [
    "What is the ROAS for each marketing channel?",
    "Show me total spend and revenue by channel",
    "Which channel has the highest attributed revenue?",
    "Compare LinkedIn spend vs Google Ads spend",
    "What is the average PMI index over time?",
    "Show weekly spend trends for the last quarter"
]


def render_example_queries(key_prefix: str = "analyst"):
    """Render clickable example queries."""
    st.markdown("**Try asking:**")
    
    cols = st.columns(2)
    for i, query in enumerate(EXAMPLE_QUERIES):
        col = cols[i % 2]
        with col:
            if st.button(f"{query}", key=f"{key_prefix}_example_{i}", use_container_width=True):
                # Set the query in session state to be picked up by chat
                st.session_state[f"{key_prefix}_suggested_query"] = query
                st.rerun()

